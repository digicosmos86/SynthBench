{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0joRksLdC8h4"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CPU as available physical device\n",
    "tf.config.experimental.set_visible_devices(devices=[], device_type=\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "nETlpCbIRDfv",
    "outputId": "d4c95d49-4ffd-4cb1-8280-73701de0b0b0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
       "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "...          ...        ...        ...       ...       ...       ...   \n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V21       V22  \\\n",
       "0       0.462388  0.239599  0.098698  0.363787  ... -0.018307  0.277838   \n",
       "1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  ...  0.247998  0.771679   \n",
       "3       1.247203  0.237609  0.377436 -1.387024  ... -0.108300  0.005274   \n",
       "4       0.095921  0.592941 -0.270533  0.817739  ... -0.009431  0.798278   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ...  0.213454  0.111864   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  0.214205  0.924384   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.232045  0.578229   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.265245  0.800049   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Amount  \\\n",
       "0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62   \n",
       "1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n",
       "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n",
       "3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n",
       "4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n",
       "...          ...       ...       ...       ...       ...       ...     ...   \n",
       "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n",
       "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n",
       "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n",
       "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n",
       "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n",
       "\n",
       "        Class  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "284802      0  \n",
       "284803      0  \n",
       "284804      0  \n",
       "284805      0  \n",
       "284806      0  \n",
       "\n",
       "[284807 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creditcard = pd.read_csv(\"../data/creditcard.csv.gz\")\n",
    "creditcard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NDuhGbsYR6KH",
    "outputId": "cde09c33-14c4-4a9d-bd23-1b6d34e9fb92"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 9.35192337e-01, 7.66490419e-01, ...,\n",
       "        3.12696634e-01, 5.82379309e-03, 0.00000000e+00],\n",
       "       [0.00000000e+00, 9.78541955e-01, 7.70066651e-01, ...,\n",
       "        3.13422663e-01, 1.04705276e-04, 0.00000000e+00],\n",
       "       [5.78730497e-06, 9.35217023e-01, 7.53117667e-01, ...,\n",
       "        3.11911316e-01, 1.47389219e-02, 0.00000000e+00],\n",
       "       ...,\n",
       "       [9.99976851e-01, 9.90904812e-01, 7.64079694e-01, ...,\n",
       "        3.12584864e-01, 2.64215395e-03, 0.00000000e+00],\n",
       "       [9.99976851e-01, 9.54208999e-01, 7.72855742e-01, ...,\n",
       "        3.15245157e-01, 3.89238944e-04, 0.00000000e+00],\n",
       "       [1.00000000e+00, 9.49231759e-01, 7.65256401e-01, ...,\n",
       "        3.13400843e-01, 8.44648509e-03, 0.00000000e+00]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_cc = MinMaxScaler()\n",
    "\n",
    "creditcard_scaled = scaler_cc.fit_transform(creditcard)\n",
    "creditcard_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "AE8uZRnLDmrt"
   },
   "outputs": [],
   "source": [
    "def make_generator(z_dim, output_dim, n_hidden=None):\n",
    "    if n_hidden is None:\n",
    "        n_hidden = int(output_dim * 4)\n",
    "\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            layers.Input(shape=(z_dim,), dtype=\"float32\", name=\"Input\"),\n",
    "            layers.Dense(\n",
    "                output_dim,\n",
    "                activation=\"tanh\",\n",
    "                name=\"Hidden_1\",\n",
    "                kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
    "            ),\n",
    "            layers.Dense(\n",
    "                n_hidden,\n",
    "                activation=\"tanh\",\n",
    "                name=\"Hidden_2\",\n",
    "                kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
    "            ),\n",
    "            layers.Dense(\n",
    "                output_dim,\n",
    "                activation=\"sigmoid\",\n",
    "                name=\"Output\",\n",
    "                kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
    "            ),\n",
    "        ],\n",
    "        name=\"generator\",\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "bfoSmj-6tBGG"
   },
   "outputs": [],
   "source": [
    "class ClipWeights(tf.keras.constraints.Constraint):\n",
    "    \"\"\"Constrains weight tensors to be centered around `ref_value`.\"\"\"\n",
    "\n",
    "    def __init__(self, min_value=-0.01, max_value=0.01):\n",
    "        self.min_value = min_value\n",
    "        self.max_value = max_value\n",
    "\n",
    "    def __call__(self, w):\n",
    "        return tf.clip_by_value(w, self.min_value, self.max_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Bftt5sb4GsE4"
   },
   "outputs": [],
   "source": [
    "def make_student_discriminator(input_dim, n_hidden=None):\n",
    "    if n_hidden is None:\n",
    "        n_hidden = int(input_dim)\n",
    "\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            layers.Input(shape=(input_dim,), dtype=\"float32\", name=\"Input\"),\n",
    "            layers.Dense(\n",
    "                n_hidden,\n",
    "                activation=\"relu\",\n",
    "                name=\"Hidden_2\",\n",
    "                kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
    "                #  kernel_constraint=ClipWeights(),\n",
    "                #  bias_constraint=ClipWeights()\n",
    "            ),\n",
    "            layers.Dense(\n",
    "                1,\n",
    "                activation=\"sigmoid\",\n",
    "                name=\"Output\",\n",
    "                kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
    "                #  kernel_constraint=ClipWeights(),\n",
    "                #  bias_constraint=ClipWeights()\n",
    "            ),\n",
    "        ],\n",
    "        name=\"student_discriminator\",\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "VYfE4G2AICwp"
   },
   "outputs": [],
   "source": [
    "def make_teacher_discriminators(n_teachers, input_dim):\n",
    "\n",
    "    models = [\n",
    "        tf.keras.Sequential(\n",
    "            [\n",
    "                layers.Input(shape=(input_dim,), dtype=\"float32\", name=\"Input\"),\n",
    "                layers.Dense(1, activation=\"sigmoid\", name=\"Output\"),\n",
    "            ],\n",
    "            name=f\"teacher_discriminator_{i}\",\n",
    "        )\n",
    "        for i in range(n_teachers)\n",
    "    ]\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "OQCh_ESMKvba"
   },
   "outputs": [],
   "source": [
    "def pate(data, teachers, lap_scale):\n",
    "\n",
    "    clean_results = tf.math.reduce_sum(\n",
    "        tf.concat(\n",
    "            [tf.cast(teacher(data) > 0.5, tf.float32) for teacher in teachers], axis=1\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    noise = np.random.laplace(\n",
    "        loc=0, scale=1 / lap_scale, size=tf.shape(clean_results).numpy()\n",
    "    )\n",
    "\n",
    "    noisy_results = clean_results + tf.constant(noise, dtype=tf.float32)\n",
    "\n",
    "    return noisy_results > len(teachers) / 2.0, clean_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "i1e9lzoSIhC8"
   },
   "outputs": [],
   "source": [
    "def moments_acc(n_teachers, clean_results, lap_scale, l_list):\n",
    "\n",
    "    q = (\n",
    "        tf.math.log(2.0 + lap_scale * tf.math.abs(2 * clean_results - n_teachers))\n",
    "        - tf.math.log(4.0)\n",
    "        - lap_scale * tf.math.abs(2.0 * clean_results - n_teachers)\n",
    "    )\n",
    "\n",
    "    q = tf.math.exp(q)\n",
    "\n",
    "    update_list = []\n",
    "\n",
    "    for l in l_list:\n",
    "        a = 2 * lap_scale * lap_scale * l * (l + 1)\n",
    "        t_one = (1 - q) * tf.math.pow((1 - q) / (1 - q * tf.math.exp(2 * lap_scale)), l)\n",
    "        t_two = q * tf.math.exp(2 * lap_scale * l)\n",
    "\n",
    "        t = tf.math.log(t_one + t_two)\n",
    "        update_list.append(tf.reduce_sum(tf.clip_by_value(t, t, a), axis=0))\n",
    "\n",
    "    return tf.concat(update_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "wTalgo52IByS"
   },
   "outputs": [],
   "source": [
    "class PATE_GAN(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self, z_dim, input_dim, n_teachers, target_epsilon, target_delta, n_hidden=None\n",
    "    ):\n",
    "        super(PATE_GAN, self).__init__()\n",
    "        self.generator = make_generator(z_dim, input_dim, n_hidden)\n",
    "        self.teachers = make_teacher_discriminators(n_teachers, input_dim)\n",
    "        self.student = make_student_discriminator(input_dim, n_hidden)\n",
    "\n",
    "        self.z_dim = z_dim\n",
    "        self.n_teachers = n_teachers\n",
    "        self.target_epsilon = target_epsilon\n",
    "        self.target_delta = target_delta\n",
    "\n",
    "    def compile(self, generator_lr=1e-4, student_lr=1e-4, teacher_lr=1e-4):\n",
    "        super(PATE_GAN, self).compile()\n",
    "        self.g_optimizer = tf.optimizers.Adam(learning_rate=generator_lr)\n",
    "        self.s_optimizer = tf.optimizers.Adam(learning_rate=student_lr)\n",
    "        self.t_optimizers = [\n",
    "            tf.optimizers.Adam(learning_rate=teacher_lr) for _ in range(self.n_teachers)\n",
    "        ]\n",
    "\n",
    "        self.loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "    def train(\n",
    "        self, X, batch_size, n_moments, lap_scale, n_teacher_iters=1, n_student_iters=1\n",
    "    ):\n",
    "        alpha = tf.zeros(n_moments, tf.float32)\n",
    "        l_list = 1 + tf.range(n_moments, dtype=tf.float32)\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "        split_size = int(n_samples / self.n_teachers)\n",
    "\n",
    "        steps = 0\n",
    "        epsilon = 0\n",
    "\n",
    "        np.random.shuffle(X)\n",
    "\n",
    "        data_splits = []\n",
    "\n",
    "        for teacher_id in range(self.n_teachers):\n",
    "            start = teacher_id * split_size\n",
    "            end = start + split_size\n",
    "\n",
    "            if teacher_id == self.n_teachers - 1:\n",
    "                end = n_samples\n",
    "\n",
    "            data_splits.append(X[start:end, :])\n",
    "\n",
    "        while epsilon < self.target_epsilon:\n",
    "\n",
    "            ### Update teacher weights with SGD\n",
    "            for t_2 in range(n_teacher_iters):\n",
    "                # Generate random noises\n",
    "\n",
    "                for i in range(self.n_teachers):\n",
    "                    current_split = data_splits[i]\n",
    "                    teacher = self.teachers[i]\n",
    "                    optimizer = self.t_optimizers[i]\n",
    "\n",
    "                    z = tf.random.uniform([batch_size, n_features], -1, 1, tf.float32)\n",
    "                    g_z = self.generator(z)\n",
    "\n",
    "                    u = tf.constant(\n",
    "                        current_split[\n",
    "                            np.random.choice(\n",
    "                                current_split.shape[0], size=batch_size, replace=False\n",
    "                            )\n",
    "                        ],\n",
    "                        dtype=tf.float32,\n",
    "                    )\n",
    "\n",
    "                    input = tf.concat([u, g_z], axis=0)\n",
    "                    y_true = tf.concat(\n",
    "                        [tf.ones(batch_size), tf.zeros(batch_size)], axis=0\n",
    "                    )\n",
    "\n",
    "                    with tf.GradientTape() as tape:\n",
    "                        y_pred = teacher(input)\n",
    "                        teacher_loss = self.loss_fn(y_true, y_pred)\n",
    "\n",
    "                    grads = tape.gradient(teacher_loss, teacher.trainable_weights)\n",
    "                    optimizer.apply_gradients(zip(grads, teacher.trainable_weights))\n",
    "\n",
    "            r_sum = 0.0\n",
    "\n",
    "            ### Update the student weight with SGD\n",
    "            for t_3 in range(n_student_iters):\n",
    "                z = tf.random.uniform([batch_size, self.z_dim], -1, 1, tf.float32)\n",
    "                u_hat = self.generator(z)\n",
    "                r, clean_results = pate(u_hat, self.teachers, lap_scale)\n",
    "                # r = r + 0.05 * tf.random.uniform(tf.shape(r), dtype=tf.float32)\n",
    "                # print(clean_results)\n",
    "\n",
    "                update = moments_acc(self.n_teachers, clean_results, lap_scale, l_list)\n",
    "                alpha = alpha + update\n",
    "\n",
    "                with tf.GradientTape() as tape:\n",
    "                    r_pred = self.student(u_hat)\n",
    "                    # print(tf.transpose(r_pred), r)\n",
    "                    student_loss = self.loss_fn(r, r_pred)\n",
    "\n",
    "                grads = tape.gradient(student_loss, self.student.trainable_weights)\n",
    "                self.s_optimizer.apply_gradients(\n",
    "                    zip(grads, self.student.trainable_weights)\n",
    "                )\n",
    "\n",
    "                r_sum = r_sum + tf.reduce_sum(tf.cast(r, dtype=tf.float32), axis=0)\n",
    "\n",
    "            ### Update the generator weights\n",
    "            z = tf.random.uniform([batch_size, self.z_dim], -1, 1, tf.float32)\n",
    "\n",
    "            y_true = tf.ones(batch_size)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                generated = self.generator(z)\n",
    "                # print(generated[0])\n",
    "                y_pred = self.student(generated)\n",
    "                generator_loss = -tf.reduce_mean(tf.math.log(y_pred))\n",
    "\n",
    "            grads = tape.gradient(generator_loss, self.generator.trainable_weights)\n",
    "            self.g_optimizer.apply_gradients(\n",
    "                zip(grads, self.generator.trainable_weights)\n",
    "            )\n",
    "            # student_weights = self.student.get_weights()\n",
    "            # self.student.set_weights([np.clip(weights, -0.1, 0.1) for weights in student_weights])\n",
    "\n",
    "            # print(f\"alpha: {alpha}\")\n",
    "\n",
    "            epsilon = tf.reduce_min((alpha - tf.math.log(self.target_delta)) / l_list)\n",
    "\n",
    "            # if steps % 100 == 0:\n",
    "            print(\n",
    "                f\"Step {steps}, Generator loss: {generator_loss.numpy()}; Student loss: {student_loss.numpy()}; Epsilon: {epsilon.numpy()}, r_sum: {r_sum.numpy()}\"\n",
    "            )\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "    def generate(self, num_rows, batch_size=1000):\n",
    "        synthetic_data = []\n",
    "\n",
    "        while num_rows > 0:\n",
    "            if num_rows < batch_size:\n",
    "                rows_to_generate = num_rows\n",
    "            else:\n",
    "                rows_to_generate = batch_size\n",
    "\n",
    "            noise = tf.random.uniform([rows_to_generate, self.z_dim], -1, 1, tf.float32)\n",
    "            synthetic_data.append(self.generator(noise).numpy())\n",
    "\n",
    "            num_rows = num_rows - batch_size\n",
    "\n",
    "        return np.concatenate(synthetic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "PgukPGRfZMHj"
   },
   "outputs": [],
   "source": [
    "pate_gan = PATE_GAN(\n",
    "    z_dim=31, input_dim=31, n_teachers=300, target_epsilon=10000, target_delta=1e-5\n",
    ")\n",
    "pate_gan.compile(student_lr=1e-4, generator_lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rmjBJ6bANcYd",
    "outputId": "c63fd83c-7fa6-4cc6-a314-119d5b4f781e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Generator loss: 0.8602196574211121; Student loss: 0.8654006719589233; Epsilon: 3.090484142303467, r_sum: 64.0\n",
      "Step 1, Generator loss: 0.8586077690124512; Student loss: 0.8434087038040161; Epsilon: 5.2555341720581055, r_sum: 61.0\n",
      "Step 2, Generator loss: 0.8531168699264526; Student loss: 0.8499969840049744; Epsilon: 5.979830741882324, r_sum: 63.0\n",
      "Step 3, Generator loss: 0.850952684879303; Student loss: 0.8627250790596008; Epsilon: 6.916790008544922, r_sum: 64.0\n",
      "Step 4, Generator loss: 0.8576366901397705; Student loss: 0.8557834029197693; Epsilon: 7.319990158081055, r_sum: 64.0\n",
      "Step 5, Generator loss: 0.8471274375915527; Student loss: 0.849330723285675; Epsilon: 8.062884330749512, r_sum: 64.0\n",
      "Step 6, Generator loss: 0.8474826216697693; Student loss: 0.850150465965271; Epsilon: 9.014281272888184, r_sum: 64.0\n",
      "Step 7, Generator loss: 0.839465320110321; Student loss: 0.841183066368103; Epsilon: 10.667141914367676, r_sum: 64.0\n",
      "Step 8, Generator loss: 0.8302038908004761; Student loss: 0.8357378244400024; Epsilon: 11.197270393371582, r_sum: 64.0\n",
      "Step 9, Generator loss: 0.832844614982605; Student loss: 0.836861252784729; Epsilon: 11.818922996520996, r_sum: 64.0\n",
      "Step 10, Generator loss: 0.8257684707641602; Student loss: 0.8185333013534546; Epsilon: 12.258567810058594, r_sum: 62.0\n",
      "Step 11, Generator loss: 0.8209965229034424; Student loss: 0.8374742269515991; Epsilon: 12.68687915802002, r_sum: 64.0\n",
      "Step 12, Generator loss: 0.8199585676193237; Student loss: 0.8338636159896851; Epsilon: 12.845992088317871, r_sum: 64.0\n",
      "Step 13, Generator loss: 0.8232530951499939; Student loss: 0.8124523162841797; Epsilon: 13.378631591796875, r_sum: 64.0\n",
      "Step 14, Generator loss: 0.8244060277938843; Student loss: 0.8221303820610046; Epsilon: 13.893596649169922, r_sum: 64.0\n",
      "Step 15, Generator loss: 0.8129010796546936; Student loss: 0.8083391189575195; Epsilon: 15.208391189575195, r_sum: 63.0\n",
      "Step 16, Generator loss: 0.809798538684845; Student loss: 0.8157218098640442; Epsilon: 15.574804306030273, r_sum: 64.0\n",
      "Step 17, Generator loss: 0.810298502445221; Student loss: 0.8100028038024902; Epsilon: 16.857166290283203, r_sum: 64.0\n",
      "Step 18, Generator loss: 0.7985477447509766; Student loss: 0.7997250556945801; Epsilon: 16.986921310424805, r_sum: 64.0\n",
      "Step 19, Generator loss: 0.7947516441345215; Student loss: 0.8025422096252441; Epsilon: 17.346956253051758, r_sum: 64.0\n",
      "Step 20, Generator loss: 0.8039513230323792; Student loss: 0.7864551544189453; Epsilon: 17.45212745666504, r_sum: 63.0\n",
      "Step 21, Generator loss: 0.7931339740753174; Student loss: 0.7922201156616211; Epsilon: 17.617090225219727, r_sum: 64.0\n",
      "Step 22, Generator loss: 0.7974547147750854; Student loss: 0.785558819770813; Epsilon: 19.05244255065918, r_sum: 63.0\n",
      "Step 23, Generator loss: 0.7875580191612244; Student loss: 0.7784898281097412; Epsilon: 19.903884887695312, r_sum: 62.0\n",
      "Step 24, Generator loss: 0.7820655107498169; Student loss: 0.7938995361328125; Epsilon: 20.121841430664062, r_sum: 64.0\n",
      "Step 25, Generator loss: 0.7836454510688782; Student loss: 0.7841877937316895; Epsilon: 20.59701919555664, r_sum: 64.0\n",
      "Step 26, Generator loss: 0.7682397961616516; Student loss: 0.7803794145584106; Epsilon: 20.757707595825195, r_sum: 64.0\n",
      "Step 27, Generator loss: 0.7719056010246277; Student loss: 0.7695304155349731; Epsilon: 21.56480598449707, r_sum: 64.0\n",
      "Step 28, Generator loss: 0.7675071954727173; Student loss: 0.7720741033554077; Epsilon: 22.05179786682129, r_sum: 64.0\n",
      "Step 29, Generator loss: 0.7698252201080322; Student loss: 0.7664128541946411; Epsilon: 23.279943466186523, r_sum: 63.0\n",
      "Step 30, Generator loss: 0.7569693326950073; Student loss: 0.7633585929870605; Epsilon: 24.059640884399414, r_sum: 63.0\n",
      "Step 31, Generator loss: 0.7657096982002258; Student loss: 0.7543364763259888; Epsilon: 24.493881225585938, r_sum: 64.0\n",
      "Step 32, Generator loss: 0.7543721795082092; Student loss: 0.7570280432701111; Epsilon: 24.676475524902344, r_sum: 64.0\n",
      "Step 33, Generator loss: 0.7541487216949463; Student loss: 0.7486248016357422; Epsilon: 24.932106018066406, r_sum: 64.0\n",
      "Step 34, Generator loss: 0.744438886642456; Student loss: 0.7527357339859009; Epsilon: 26.282258987426758, r_sum: 63.0\n",
      "Step 35, Generator loss: 0.7525217533111572; Student loss: 0.752691388130188; Epsilon: 27.568788528442383, r_sum: 64.0\n",
      "Step 36, Generator loss: 0.7486542463302612; Student loss: 0.742583155632019; Epsilon: 28.481765747070312, r_sum: 64.0\n",
      "Step 37, Generator loss: 0.7513670921325684; Student loss: 0.7471514344215393; Epsilon: 29.54684829711914, r_sum: 64.0\n",
      "Step 38, Generator loss: 0.7335019707679749; Student loss: 0.7412183284759521; Epsilon: 31.69049835205078, r_sum: 64.0\n",
      "Step 39, Generator loss: 0.7411177158355713; Student loss: 0.7358351945877075; Epsilon: 32.12889862060547, r_sum: 63.0\n",
      "Step 40, Generator loss: 0.7336610555648804; Student loss: 0.7295168042182922; Epsilon: 33.75290298461914, r_sum: 64.0\n",
      "Step 41, Generator loss: 0.7233819961547852; Student loss: 0.732966423034668; Epsilon: 34.22850799560547, r_sum: 64.0\n",
      "Step 42, Generator loss: 0.7228177785873413; Student loss: 0.7273648977279663; Epsilon: 34.908447265625, r_sum: 64.0\n",
      "Step 43, Generator loss: 0.7157368659973145; Student loss: 0.7235066294670105; Epsilon: 35.97902297973633, r_sum: 63.0\n",
      "Step 44, Generator loss: 0.7172418832778931; Student loss: 0.7163609266281128; Epsilon: 36.22837829589844, r_sum: 64.0\n",
      "Step 45, Generator loss: 0.7082505822181702; Student loss: 0.7070446610450745; Epsilon: 37.562889099121094, r_sum: 62.0\n",
      "Step 46, Generator loss: 0.7060900926589966; Student loss: 0.7174091339111328; Epsilon: 37.86166763305664, r_sum: 64.0\n",
      "Step 47, Generator loss: 0.7055842280387878; Student loss: 0.7077497243881226; Epsilon: 38.338897705078125, r_sum: 63.0\n",
      "Step 48, Generator loss: 0.7036657333374023; Student loss: 0.7030299305915833; Epsilon: 39.808319091796875, r_sum: 63.0\n",
      "Step 49, Generator loss: 0.7019929885864258; Student loss: 0.700127363204956; Epsilon: 39.99448776245117, r_sum: 63.0\n",
      "Step 50, Generator loss: 0.6942043900489807; Student loss: 0.7015419006347656; Epsilon: 42.98503112792969, r_sum: 63.0\n",
      "Step 51, Generator loss: 0.6980515718460083; Student loss: 0.6966584920883179; Epsilon: 43.423954010009766, r_sum: 64.0\n",
      "Step 52, Generator loss: 0.6973370313644409; Student loss: 0.689487099647522; Epsilon: 44.453182220458984, r_sum: 63.0\n",
      "Step 53, Generator loss: 0.6880038380622864; Student loss: 0.6968018412590027; Epsilon: 44.17615509033203, r_sum: 63.0\n",
      "Step 54, Generator loss: 0.6875709295272827; Student loss: 0.6826112270355225; Epsilon: 44.62767028808594, r_sum: 63.0\n",
      "Step 55, Generator loss: 0.6743305325508118; Student loss: 0.6821302175521851; Epsilon: 45.62722396850586, r_sum: 64.0\n",
      "Step 56, Generator loss: 0.6732234954833984; Student loss: 0.6803359985351562; Epsilon: 46.08930587768555, r_sum: 64.0\n",
      "Step 57, Generator loss: 0.6786019802093506; Student loss: 0.6783168315887451; Epsilon: 46.741981506347656, r_sum: 63.0\n",
      "Step 58, Generator loss: 0.6703473329544067; Student loss: 0.6748542785644531; Epsilon: 49.59452819824219, r_sum: 63.0\n",
      "Step 59, Generator loss: 0.6734710931777954; Student loss: 0.671902596950531; Epsilon: 49.27586364746094, r_sum: 64.0\n",
      "Step 60, Generator loss: 0.6621222496032715; Student loss: 0.6739591360092163; Epsilon: 49.400665283203125, r_sum: 64.0\n",
      "Step 61, Generator loss: 0.6688058376312256; Student loss: 0.6650205254554749; Epsilon: 50.310081481933594, r_sum: 63.0\n",
      "Step 62, Generator loss: 0.6531146764755249; Student loss: 0.668166995048523; Epsilon: 50.46043395996094, r_sum: 63.0\n",
      "Step 63, Generator loss: 0.6596757769584656; Student loss: 0.6528775095939636; Epsilon: 50.9935188293457, r_sum: 64.0\n",
      "Step 64, Generator loss: 0.6470776796340942; Student loss: 0.6597291231155396; Epsilon: 52.02080535888672, r_sum: 62.0\n",
      "Step 65, Generator loss: 0.6443142890930176; Student loss: 0.6518766283988953; Epsilon: 54.20821762084961, r_sum: 63.0\n",
      "Step 66, Generator loss: 0.6465779542922974; Student loss: 0.6567997932434082; Epsilon: 54.91570281982422, r_sum: 63.0\n",
      "Step 67, Generator loss: 0.6432215571403503; Student loss: 0.6528488397598267; Epsilon: 57.49779510498047, r_sum: 62.0\n",
      "Step 68, Generator loss: 0.6385163068771362; Student loss: 0.6465927958488464; Epsilon: 57.759063720703125, r_sum: 63.0\n",
      "Step 69, Generator loss: 0.6338561177253723; Student loss: 0.6413557529449463; Epsilon: 59.280582427978516, r_sum: 64.0\n",
      "Step 70, Generator loss: 0.6386091709136963; Student loss: 0.6383000612258911; Epsilon: 60.187931060791016, r_sum: 63.0\n",
      "Step 71, Generator loss: 0.6297600269317627; Student loss: 0.636508047580719; Epsilon: 60.722015380859375, r_sum: 64.0\n",
      "Step 72, Generator loss: 0.6331189274787903; Student loss: 0.6349270343780518; Epsilon: 62.63228988647461, r_sum: 63.0\n",
      "Step 73, Generator loss: 0.6264033913612366; Student loss: 0.6253255605697632; Epsilon: 64.88390350341797, r_sum: 62.0\n",
      "Step 74, Generator loss: 0.6235988140106201; Student loss: 0.6293869018554688; Epsilon: 66.34205627441406, r_sum: 60.0\n",
      "Step 75, Generator loss: 0.6231185793876648; Student loss: 0.6211018562316895; Epsilon: 68.81639862060547, r_sum: 61.0\n",
      "Step 76, Generator loss: 0.6205103397369385; Student loss: 0.6250675916671753; Epsilon: 70.2564468383789, r_sum: 63.0\n",
      "Step 77, Generator loss: 0.6104709506034851; Student loss: 0.6232802867889404; Epsilon: 71.36406707763672, r_sum: 63.0\n",
      "Step 78, Generator loss: 0.6138428449630737; Student loss: 0.6164683699607849; Epsilon: 73.06023406982422, r_sum: 64.0\n",
      "Step 79, Generator loss: 0.6087995767593384; Student loss: 0.6096118688583374; Epsilon: 74.57804870605469, r_sum: 63.0\n",
      "Step 80, Generator loss: 0.6068331003189087; Student loss: 0.6203681230545044; Epsilon: 75.83564758300781, r_sum: 62.0\n",
      "Step 81, Generator loss: 0.6030322313308716; Student loss: 0.6083024740219116; Epsilon: 77.17422485351562, r_sum: 63.0\n",
      "Step 82, Generator loss: 0.5911794304847717; Student loss: 0.5990327596664429; Epsilon: 78.92569732666016, r_sum: 63.0\n",
      "Step 83, Generator loss: 0.593242883682251; Student loss: 0.5936733484268188; Epsilon: 79.77423095703125, r_sum: 64.0\n",
      "Step 84, Generator loss: 0.5902374982833862; Student loss: 0.5937667489051819; Epsilon: 80.97540283203125, r_sum: 63.0\n",
      "Step 85, Generator loss: 0.5962058901786804; Student loss: 0.5902458429336548; Epsilon: 82.4282455444336, r_sum: 63.0\n",
      "Step 86, Generator loss: 0.5888906717300415; Student loss: 0.5940559506416321; Epsilon: 84.41407012939453, r_sum: 62.0\n",
      "Step 87, Generator loss: 0.5862516760826111; Student loss: 0.5963168144226074; Epsilon: 84.82572174072266, r_sum: 63.0\n",
      "Step 88, Generator loss: 0.5756187438964844; Student loss: 0.5879606008529663; Epsilon: 89.65642547607422, r_sum: 63.0\n",
      "Step 89, Generator loss: 0.5730352401733398; Student loss: 0.5889105796813965; Epsilon: 91.20578002929688, r_sum: 62.0\n",
      "Step 90, Generator loss: 0.5712651610374451; Student loss: 0.5903418660163879; Epsilon: 94.79873657226562, r_sum: 62.0\n",
      "Step 91, Generator loss: 0.5725880861282349; Student loss: 0.5782444477081299; Epsilon: 96.64029693603516, r_sum: 64.0\n",
      "Step 92, Generator loss: 0.5769018530845642; Student loss: 0.579502284526825; Epsilon: 99.92547607421875, r_sum: 63.0\n",
      "Step 93, Generator loss: 0.5636137127876282; Student loss: 0.5844482183456421; Epsilon: 103.64322662353516, r_sum: 61.0\n",
      "Step 94, Generator loss: 0.571998119354248; Student loss: 0.5687093734741211; Epsilon: 104.81197357177734, r_sum: 63.0\n",
      "Step 95, Generator loss: 0.555322527885437; Student loss: 0.5727993249893188; Epsilon: 106.81678009033203, r_sum: 62.0\n",
      "Step 96, Generator loss: 0.5660239458084106; Student loss: 0.5826107263565063; Epsilon: 110.41938018798828, r_sum: 61.0\n",
      "Step 97, Generator loss: 0.5596485137939453; Student loss: 0.5631942749023438; Epsilon: 111.28703308105469, r_sum: 63.0\n",
      "Step 98, Generator loss: 0.5490643382072449; Student loss: 0.553982138633728; Epsilon: 113.7452621459961, r_sum: 64.0\n",
      "Step 99, Generator loss: 0.561852216720581; Student loss: 0.5540652871131897; Epsilon: 114.26606750488281, r_sum: 63.0\n",
      "Step 100, Generator loss: 0.5512662529945374; Student loss: 0.572891116142273; Epsilon: 116.9922103881836, r_sum: 60.0\n",
      "Step 101, Generator loss: 0.549609899520874; Student loss: 0.5494725108146667; Epsilon: 119.02831268310547, r_sum: 63.0\n",
      "Step 102, Generator loss: 0.5462253093719482; Student loss: 0.5584787130355835; Epsilon: 121.46902465820312, r_sum: 61.0\n",
      "Step 103, Generator loss: 0.544177770614624; Student loss: 0.5499837398529053; Epsilon: 123.371826171875, r_sum: 64.0\n",
      "Step 104, Generator loss: 0.5445156097412109; Student loss: 0.5355879664421082; Epsilon: 123.5338363647461, r_sum: 64.0\n",
      "Step 105, Generator loss: 0.5455547571182251; Student loss: 0.53719562292099; Epsilon: 125.07254028320312, r_sum: 63.0\n",
      "Step 106, Generator loss: 0.5321219563484192; Student loss: 0.5571857690811157; Epsilon: 127.5322036743164, r_sum: 61.0\n",
      "Step 107, Generator loss: 0.5321317911148071; Student loss: 0.5453289151191711; Epsilon: 130.24610900878906, r_sum: 61.0\n",
      "Step 108, Generator loss: 0.526136040687561; Student loss: 0.5403081774711609; Epsilon: 132.56056213378906, r_sum: 63.0\n",
      "Step 109, Generator loss: 0.5255803465843201; Student loss: 0.5399177074432373; Epsilon: 135.70631408691406, r_sum: 63.0\n",
      "Step 110, Generator loss: 0.5251210927963257; Student loss: 0.5403821468353271; Epsilon: 137.3175811767578, r_sum: 62.0\n",
      "Step 111, Generator loss: 0.5300257205963135; Student loss: 0.5289915800094604; Epsilon: 140.7835693359375, r_sum: 63.0\n",
      "Step 112, Generator loss: 0.5251718759536743; Student loss: 0.5472250580787659; Epsilon: 144.35479736328125, r_sum: 60.0\n",
      "Step 113, Generator loss: 0.5218788385391235; Student loss: 0.5355319976806641; Epsilon: 145.60885620117188, r_sum: 62.0\n",
      "Step 114, Generator loss: 0.5207856297492981; Student loss: 0.5234271883964539; Epsilon: 147.57122802734375, r_sum: 63.0\n",
      "Step 115, Generator loss: 0.5148216485977173; Student loss: 0.5390764474868774; Epsilon: 149.40823364257812, r_sum: 61.0\n",
      "Step 116, Generator loss: 0.5086306929588318; Student loss: 0.5163165330886841; Epsilon: 152.46435546875, r_sum: 63.0\n",
      "Step 117, Generator loss: 0.5018240213394165; Student loss: 0.5224894285202026; Epsilon: 156.02467346191406, r_sum: 62.0\n",
      "Step 118, Generator loss: 0.4970167875289917; Student loss: 0.5260505080223083; Epsilon: 159.3300323486328, r_sum: 60.0\n",
      "Step 119, Generator loss: 0.504572868347168; Student loss: 0.5347046852111816; Epsilon: 163.50625610351562, r_sum: 59.0\n",
      "Step 120, Generator loss: 0.5057381391525269; Student loss: 0.5295189619064331; Epsilon: 166.68511962890625, r_sum: 60.0\n",
      "Step 121, Generator loss: 0.5022976398468018; Student loss: 0.5033829212188721; Epsilon: 169.2437286376953, r_sum: 63.0\n",
      "Step 122, Generator loss: 0.49094158411026; Student loss: 0.5331034660339355; Epsilon: 172.8203582763672, r_sum: 59.0\n",
      "Step 123, Generator loss: 0.48972222208976746; Student loss: 0.5262464284896851; Epsilon: 178.47265625, r_sum: 59.0\n",
      "Step 124, Generator loss: 0.48964327573776245; Student loss: 0.5106324553489685; Epsilon: 182.05638122558594, r_sum: 62.0\n",
      "Step 125, Generator loss: 0.4888559579849243; Student loss: 0.4977957308292389; Epsilon: 184.5266571044922, r_sum: 63.0\n",
      "Step 126, Generator loss: 0.49296140670776367; Student loss: 0.5238360166549683; Epsilon: 188.74876403808594, r_sum: 59.0\n",
      "Step 127, Generator loss: 0.47591015696525574; Student loss: 0.497999906539917; Epsilon: 190.33401489257812, r_sum: 64.0\n",
      "Step 128, Generator loss: 0.4828666150569916; Student loss: 0.5029894113540649; Epsilon: 192.7528076171875, r_sum: 62.0\n",
      "Step 129, Generator loss: 0.47622379660606384; Student loss: 0.4829491376876831; Epsilon: 195.4757537841797, r_sum: 63.0\n",
      "Step 130, Generator loss: 0.4786132574081421; Student loss: 0.49613744020462036; Epsilon: 198.1751251220703, r_sum: 62.0\n",
      "Step 131, Generator loss: 0.4794028103351593; Student loss: 0.48398298025131226; Epsilon: 203.18533325195312, r_sum: 64.0\n",
      "Step 132, Generator loss: 0.46942660212516785; Student loss: 0.48094168305397034; Epsilon: 206.29440307617188, r_sum: 64.0\n",
      "Step 133, Generator loss: 0.47608721256256104; Student loss: 0.5218679308891296; Epsilon: 211.77711486816406, r_sum: 58.0\n",
      "Step 134, Generator loss: 0.46536892652511597; Student loss: 0.48372969031333923; Epsilon: 211.70025634765625, r_sum: 62.0\n",
      "Step 135, Generator loss: 0.4585278034210205; Student loss: 0.4922489523887634; Epsilon: 214.08546447753906, r_sum: 61.0\n",
      "Step 136, Generator loss: 0.46713706851005554; Student loss: 0.49138984084129333; Epsilon: 218.33627319335938, r_sum: 61.0\n",
      "Step 137, Generator loss: 0.4705186188220978; Student loss: 0.5053877830505371; Epsilon: 222.1721649169922, r_sum: 58.0\n",
      "Step 138, Generator loss: 0.4588659405708313; Student loss: 0.5292212963104248; Epsilon: 226.302734375, r_sum: 56.0\n",
      "Step 139, Generator loss: 0.4572647213935852; Student loss: 0.5255232453346252; Epsilon: 232.81874084472656, r_sum: 56.0\n",
      "Step 140, Generator loss: 0.4525168836116791; Student loss: 0.5001133680343628; Epsilon: 235.8946990966797, r_sum: 59.0\n",
      "Step 141, Generator loss: 0.4508826732635498; Student loss: 0.5068867206573486; Epsilon: 241.49937438964844, r_sum: 58.0\n",
      "Step 142, Generator loss: 0.4448089599609375; Student loss: 0.477378785610199; Epsilon: 247.93142700195312, r_sum: 61.0\n",
      "Step 143, Generator loss: 0.4515851140022278; Student loss: 0.4677586555480957; Epsilon: 250.78482055664062, r_sum: 62.0\n",
      "Step 144, Generator loss: 0.45555004477500916; Student loss: 0.48803818225860596; Epsilon: 253.11801147460938, r_sum: 60.0\n",
      "Step 145, Generator loss: 0.45122772455215454; Student loss: 0.46088898181915283; Epsilon: 254.27484130859375, r_sum: 63.0\n",
      "Step 146, Generator loss: 0.44843462109565735; Student loss: 0.48149603605270386; Epsilon: 258.6646728515625, r_sum: 61.0\n",
      "Step 147, Generator loss: 0.4450191259384155; Student loss: 0.47897106409072876; Epsilon: 263.9066467285156, r_sum: 60.0\n",
      "Step 148, Generator loss: 0.43824708461761475; Student loss: 0.4748850166797638; Epsilon: 270.5932922363281, r_sum: 60.0\n",
      "Step 149, Generator loss: 0.44032585620880127; Student loss: 0.5210261940956116; Epsilon: 278.5391540527344, r_sum: 55.0\n",
      "Step 150, Generator loss: 0.4459264874458313; Student loss: 0.4543948769569397; Epsilon: 281.4813232421875, r_sum: 62.0\n",
      "Step 151, Generator loss: 0.43118101358413696; Student loss: 0.483026385307312; Epsilon: 285.9716491699219, r_sum: 59.0\n",
      "Step 152, Generator loss: 0.4307789206504822; Student loss: 0.4856122136116028; Epsilon: 291.9747009277344, r_sum: 58.0\n",
      "Step 153, Generator loss: 0.4366350769996643; Student loss: 0.4632183313369751; Epsilon: 294.9730529785156, r_sum: 61.0\n",
      "Step 154, Generator loss: 0.4375537037849426; Student loss: 0.49498605728149414; Epsilon: 304.89910888671875, r_sum: 58.0\n",
      "Step 155, Generator loss: 0.4227827489376068; Student loss: 0.47172635793685913; Epsilon: 308.9508361816406, r_sum: 59.0\n",
      "Step 156, Generator loss: 0.43023282289505005; Student loss: 0.49074146151542664; Epsilon: 315.4983215332031, r_sum: 57.0\n",
      "Step 157, Generator loss: 0.42265015840530396; Student loss: 0.447378933429718; Epsilon: 323.38848876953125, r_sum: 61.0\n",
      "Step 158, Generator loss: 0.4180816411972046; Student loss: 0.4275844097137451; Epsilon: 327.8143005371094, r_sum: 63.0\n",
      "Step 159, Generator loss: 0.4219033718109131; Student loss: 0.45604461431503296; Epsilon: 334.4739074707031, r_sum: 61.0\n",
      "Step 160, Generator loss: 0.41020283102989197; Student loss: 0.4622468948364258; Epsilon: 339.8623352050781, r_sum: 59.0\n",
      "Step 161, Generator loss: 0.41969916224479675; Student loss: 0.47333580255508423; Epsilon: 343.9411315917969, r_sum: 58.0\n",
      "Step 162, Generator loss: 0.41269704699516296; Student loss: 0.4344424605369568; Epsilon: 345.9020690917969, r_sum: 62.0\n",
      "Step 163, Generator loss: 0.41562578082084656; Student loss: 0.4688692092895508; Epsilon: 348.0725402832031, r_sum: 58.0\n",
      "Step 164, Generator loss: 0.40367794036865234; Student loss: 0.4829343557357788; Epsilon: 351.6758728027344, r_sum: 57.0\n",
      "Step 165, Generator loss: 0.4054989218711853; Student loss: 0.47811466455459595; Epsilon: 357.5975646972656, r_sum: 57.0\n",
      "Step 166, Generator loss: 0.4019535183906555; Student loss: 0.4676797688007355; Epsilon: 361.5343933105469, r_sum: 58.0\n",
      "Step 167, Generator loss: 0.39475569128990173; Student loss: 0.45392078161239624; Epsilon: 363.07403564453125, r_sum: 59.0\n",
      "Step 168, Generator loss: 0.3947330415248871; Student loss: 0.4835717976093292; Epsilon: 370.75439453125, r_sum: 56.0\n",
      "Step 169, Generator loss: 0.4079105257987976; Student loss: 0.4485616385936737; Epsilon: 380.2098083496094, r_sum: 60.0\n",
      "Step 170, Generator loss: 0.4029521942138672; Student loss: 0.4554835259914398; Epsilon: 384.9515686035156, r_sum: 58.0\n",
      "Step 171, Generator loss: 0.40103134512901306; Student loss: 0.5072041749954224; Epsilon: 390.2936706542969, r_sum: 54.0\n",
      "Step 172, Generator loss: 0.39973679184913635; Student loss: 0.4453013837337494; Epsilon: 394.72930908203125, r_sum: 59.0\n",
      "Step 173, Generator loss: 0.3855935037136078; Student loss: 0.46527791023254395; Epsilon: 399.2095947265625, r_sum: 58.0\n",
      "Step 174, Generator loss: 0.39351946115493774; Student loss: 0.5008138418197632; Epsilon: 410.8928527832031, r_sum: 54.0\n",
      "Step 175, Generator loss: 0.38680630922317505; Student loss: 0.4521598815917969; Epsilon: 419.9909973144531, r_sum: 58.0\n",
      "Step 176, Generator loss: 0.3954234719276428; Student loss: 0.4666396379470825; Epsilon: 426.29931640625, r_sum: 57.0\n",
      "Step 177, Generator loss: 0.38616472482681274; Student loss: 0.46417054533958435; Epsilon: 433.4324951171875, r_sum: 57.0\n",
      "Step 178, Generator loss: 0.37178584933280945; Student loss: 0.4486773610115051; Epsilon: 442.1792907714844, r_sum: 58.0\n",
      "Step 179, Generator loss: 0.3831484019756317; Student loss: 0.49387800693511963; Epsilon: 448.416015625, r_sum: 54.0\n",
      "Step 180, Generator loss: 0.37667375802993774; Student loss: 0.48235926032066345; Epsilon: 454.8199462890625, r_sum: 54.0\n",
      "Step 181, Generator loss: 0.38062626123428345; Student loss: 0.47920793294906616; Epsilon: 463.8340759277344, r_sum: 55.0\n",
      "Step 182, Generator loss: 0.37259089946746826; Student loss: 0.4276483356952667; Epsilon: 470.54998779296875, r_sum: 59.0\n",
      "Step 183, Generator loss: 0.3704454302787781; Student loss: 0.46149203181266785; Epsilon: 482.54217529296875, r_sum: 56.0\n",
      "Step 184, Generator loss: 0.3704780638217926; Student loss: 0.4567136764526367; Epsilon: 487.0272521972656, r_sum: 57.0\n",
      "Step 185, Generator loss: 0.3621281385421753; Student loss: 0.45800530910491943; Epsilon: 495.4638366699219, r_sum: 56.0\n",
      "Step 186, Generator loss: 0.3686479926109314; Student loss: 0.5198803544044495; Epsilon: 505.46270751953125, r_sum: 51.0\n",
      "Step 187, Generator loss: 0.37490272521972656; Student loss: 0.5012125968933105; Epsilon: 516.3411254882812, r_sum: 53.0\n",
      "Step 188, Generator loss: 0.36549270153045654; Student loss: 0.5518134832382202; Epsilon: 523.6298828125, r_sum: 48.0\n",
      "Step 189, Generator loss: 0.35746926069259644; Student loss: 0.46590861678123474; Epsilon: 525.599853515625, r_sum: 55.0\n",
      "Step 190, Generator loss: 0.356330931186676; Student loss: 0.506595253944397; Epsilon: 536.3588256835938, r_sum: 52.0\n",
      "Step 191, Generator loss: 0.36069053411483765; Student loss: 0.5470563173294067; Epsilon: 545.968017578125, r_sum: 49.0\n",
      "Step 192, Generator loss: 0.34761351346969604; Student loss: 0.4519590735435486; Epsilon: 554.3557739257812, r_sum: 56.0\n",
      "Step 193, Generator loss: 0.359456330537796; Student loss: 0.48530128598213196; Epsilon: 562.4703369140625, r_sum: 54.0\n",
      "Step 194, Generator loss: 0.36349523067474365; Student loss: 0.4825579524040222; Epsilon: 572.3634033203125, r_sum: 54.0\n",
      "Step 195, Generator loss: 0.35342371463775635; Student loss: 0.5120618343353271; Epsilon: 577.5009765625, r_sum: 51.0\n",
      "Step 196, Generator loss: 0.3394291400909424; Student loss: 0.5546948313713074; Epsilon: 587.7743530273438, r_sum: 48.0\n",
      "Step 197, Generator loss: 0.3500942587852478; Student loss: 0.5225833058357239; Epsilon: 596.3553466796875, r_sum: 50.0\n",
      "Step 198, Generator loss: 0.3423546552658081; Student loss: 0.45881664752960205; Epsilon: 604.6995849609375, r_sum: 55.0\n",
      "Step 199, Generator loss: 0.3522188663482666; Student loss: 0.4897533059120178; Epsilon: 613.8789672851562, r_sum: 53.0\n",
      "Step 200, Generator loss: 0.34278491139411926; Student loss: 0.48570311069488525; Epsilon: 625.4592895507812, r_sum: 53.0\n",
      "Step 201, Generator loss: 0.3487837612628937; Student loss: 0.44780826568603516; Epsilon: 631.1173095703125, r_sum: 55.0\n",
      "Step 202, Generator loss: 0.3435095548629761; Student loss: 0.44542771577835083; Epsilon: 638.2819213867188, r_sum: 56.0\n",
      "Step 203, Generator loss: 0.3408903479576111; Student loss: 0.5245523452758789; Epsilon: 644.0714111328125, r_sum: 50.0\n",
      "Step 204, Generator loss: 0.3467448949813843; Student loss: 0.4049757122993469; Epsilon: 651.2258911132812, r_sum: 58.0\n",
      "Step 205, Generator loss: 0.3344380855560303; Student loss: 0.5303687453269958; Epsilon: 666.536376953125, r_sum: 50.0\n",
      "Step 206, Generator loss: 0.3331916928291321; Student loss: 0.4867621958255768; Epsilon: 676.7379760742188, r_sum: 53.0\n",
      "Step 207, Generator loss: 0.34034448862075806; Student loss: 0.511915922164917; Epsilon: 685.595703125, r_sum: 50.0\n",
      "Step 208, Generator loss: 0.341448038816452; Student loss: 0.5273993015289307; Epsilon: 699.0580444335938, r_sum: 50.0\n",
      "Step 209, Generator loss: 0.3439089059829712; Student loss: 0.488700807094574; Epsilon: 705.8133544921875, r_sum: 52.0\n",
      "Step 210, Generator loss: 0.33126965165138245; Student loss: 0.4905032515525818; Epsilon: 714.484619140625, r_sum: 52.0\n",
      "Step 211, Generator loss: 0.32769495248794556; Student loss: 0.5133705735206604; Epsilon: 722.95263671875, r_sum: 49.0\n",
      "Step 212, Generator loss: 0.3355649709701538; Student loss: 0.5515904426574707; Epsilon: 735.7155151367188, r_sum: 48.0\n",
      "Step 213, Generator loss: 0.3397178649902344; Student loss: 0.41094112396240234; Epsilon: 740.2484741210938, r_sum: 57.0\n",
      "Step 214, Generator loss: 0.32557111978530884; Student loss: 0.4948362112045288; Epsilon: 748.6947631835938, r_sum: 52.0\n",
      "Step 215, Generator loss: 0.3278253674507141; Student loss: 0.4790499806404114; Epsilon: 756.074462890625, r_sum: 53.0\n",
      "Step 216, Generator loss: 0.32884615659713745; Student loss: 0.5353297591209412; Epsilon: 766.5269775390625, r_sum: 49.0\n",
      "Step 217, Generator loss: 0.31796514987945557; Student loss: 0.5080215930938721; Epsilon: 774.43994140625, r_sum: 50.0\n",
      "Step 218, Generator loss: 0.3297552466392517; Student loss: 0.4767046570777893; Epsilon: 783.9210205078125, r_sum: 53.0\n",
      "Step 219, Generator loss: 0.32749801874160767; Student loss: 0.444594144821167; Epsilon: 794.7810668945312, r_sum: 55.0\n",
      "Step 220, Generator loss: 0.3228248953819275; Student loss: 0.4979626536369324; Epsilon: 801.3704833984375, r_sum: 51.0\n",
      "Step 221, Generator loss: 0.33260324597358704; Student loss: 0.6022678017616272; Epsilon: 811.0861206054688, r_sum: 44.0\n",
      "Step 222, Generator loss: 0.32307958602905273; Student loss: 0.5133482217788696; Epsilon: 817.8178100585938, r_sum: 50.0\n",
      "Step 223, Generator loss: 0.33203867077827454; Student loss: 0.5938143134117126; Epsilon: 831.1112060546875, r_sum: 44.0\n",
      "Step 224, Generator loss: 0.3080151677131653; Student loss: 0.47494998574256897; Epsilon: 840.7962646484375, r_sum: 53.0\n",
      "Step 225, Generator loss: 0.3171918988227844; Student loss: 0.47522443532943726; Epsilon: 850.352783203125, r_sum: 52.0\n",
      "Step 226, Generator loss: 0.31896859407424927; Student loss: 0.5172183513641357; Epsilon: 861.84228515625, r_sum: 50.0\n",
      "Step 227, Generator loss: 0.31607115268707275; Student loss: 0.5846944451332092; Epsilon: 868.2044677734375, r_sum: 45.0\n",
      "Step 228, Generator loss: 0.3205299973487854; Student loss: 0.4936557114124298; Epsilon: 876.493896484375, r_sum: 51.0\n",
      "Step 229, Generator loss: 0.31649574637413025; Student loss: 0.4941265285015106; Epsilon: 886.72705078125, r_sum: 51.0\n",
      "Step 230, Generator loss: 0.30995383858680725; Student loss: 0.5763734579086304; Epsilon: 899.5748901367188, r_sum: 46.0\n",
      "Step 231, Generator loss: 0.3198283016681671; Student loss: 0.6728342771530151; Epsilon: 912.7921142578125, r_sum: 39.0\n",
      "Step 232, Generator loss: 0.3065112829208374; Student loss: 0.5358278155326843; Epsilon: 915.550537109375, r_sum: 49.0\n",
      "Step 233, Generator loss: 0.3137427568435669; Student loss: 0.5267052054405212; Epsilon: 927.7360229492188, r_sum: 49.0\n",
      "Step 234, Generator loss: 0.3161900043487549; Student loss: 0.5359915494918823; Epsilon: 940.6253051757812, r_sum: 48.0\n",
      "Step 235, Generator loss: 0.3052598237991333; Student loss: 0.5768963098526001; Epsilon: 951.2913818359375, r_sum: 46.0\n",
      "Step 236, Generator loss: 0.311565637588501; Student loss: 0.5670784711837769; Epsilon: 963.2493286132812, r_sum: 46.0\n",
      "Step 237, Generator loss: 0.29772329330444336; Student loss: 0.7187456488609314; Epsilon: 976.40185546875, r_sum: 36.0\n",
      "Step 238, Generator loss: 0.30985236167907715; Student loss: 0.5978783369064331; Epsilon: 989.384521484375, r_sum: 44.0\n",
      "Step 239, Generator loss: 0.31479066610336304; Student loss: 0.5460675358772278; Epsilon: 995.0381469726562, r_sum: 47.0\n",
      "Step 240, Generator loss: 0.314643919467926; Student loss: 0.6359447836875916; Epsilon: 1007.7299194335938, r_sum: 42.0\n",
      "Step 241, Generator loss: 0.300548791885376; Student loss: 0.5974279046058655; Epsilon: 1018.0698852539062, r_sum: 43.0\n",
      "Step 242, Generator loss: 0.30151456594467163; Student loss: 0.5831873416900635; Epsilon: 1032.76123046875, r_sum: 45.0\n",
      "Step 243, Generator loss: 0.3199649453163147; Student loss: 0.5040723085403442; Epsilon: 1044.10302734375, r_sum: 51.0\n",
      "Step 244, Generator loss: 0.30544450879096985; Student loss: 0.6360678672790527; Epsilon: 1056.7789306640625, r_sum: 42.0\n",
      "Step 245, Generator loss: 0.31016698479652405; Student loss: 0.640485405921936; Epsilon: 1070.0379638671875, r_sum: 42.0\n",
      "Step 246, Generator loss: 0.3115859627723694; Student loss: 0.6448516845703125; Epsilon: 1081.602294921875, r_sum: 42.0\n",
      "Step 247, Generator loss: 0.32209086418151855; Student loss: 0.6406112909317017; Epsilon: 1096.2681884765625, r_sum: 42.0\n",
      "Step 248, Generator loss: 0.3137412965297699; Student loss: 0.5927748680114746; Epsilon: 1108.1533203125, r_sum: 45.0\n",
      "Step 249, Generator loss: 0.31404587626457214; Student loss: 0.663084864616394; Epsilon: 1122.44140625, r_sum: 40.0\n",
      "Step 250, Generator loss: 0.30639320611953735; Student loss: 0.5676583051681519; Epsilon: 1132.1541748046875, r_sum: 46.0\n",
      "Step 251, Generator loss: 0.30947721004486084; Student loss: 0.5992053747177124; Epsilon: 1145.9892578125, r_sum: 44.0\n",
      "Step 252, Generator loss: 0.3051370680332184; Student loss: 0.5500123500823975; Epsilon: 1155.2269287109375, r_sum: 47.0\n",
      "Step 253, Generator loss: 0.30633270740509033; Student loss: 0.5836902856826782; Epsilon: 1169.2479248046875, r_sum: 46.0\n",
      "Step 254, Generator loss: 0.31561946868896484; Student loss: 0.4972139000892639; Epsilon: 1176.3544921875, r_sum: 51.0\n",
      "Step 255, Generator loss: 0.309842050075531; Student loss: 0.6931495666503906; Epsilon: 1191.84375, r_sum: 39.0\n",
      "Step 256, Generator loss: 0.3115232586860657; Student loss: 0.6252245903015137; Epsilon: 1202.553955078125, r_sum: 43.0\n",
      "Step 257, Generator loss: 0.31137603521347046; Student loss: 0.6869295835494995; Epsilon: 1213.7987060546875, r_sum: 39.0\n",
      "Step 258, Generator loss: 0.3106381893157959; Student loss: 0.7082293629646301; Epsilon: 1228.5238037109375, r_sum: 37.0\n",
      "Step 259, Generator loss: 0.318046510219574; Student loss: 0.7720954418182373; Epsilon: 1243.5748291015625, r_sum: 33.0\n",
      "Step 260, Generator loss: 0.3129998445510864; Student loss: 0.6921113729476929; Epsilon: 1256.154052734375, r_sum: 39.0\n",
      "Step 261, Generator loss: 0.3146969676017761; Student loss: 0.6693519949913025; Epsilon: 1266.3275146484375, r_sum: 40.0\n",
      "Step 262, Generator loss: 0.31896549463272095; Student loss: 0.772059440612793; Epsilon: 1278.672119140625, r_sum: 32.0\n",
      "Step 263, Generator loss: 0.30667954683303833; Student loss: 0.6650650501251221; Epsilon: 1295.3740234375, r_sum: 40.0\n",
      "Step 264, Generator loss: 0.31347811222076416; Student loss: 0.7122188210487366; Epsilon: 1304.657470703125, r_sum: 37.0\n",
      "Step 265, Generator loss: 0.3147090673446655; Student loss: 0.6885916590690613; Epsilon: 1314.86083984375, r_sum: 38.0\n",
      "Step 266, Generator loss: 0.3116190731525421; Student loss: 0.6972492337226868; Epsilon: 1323.736328125, r_sum: 37.0\n",
      "Step 267, Generator loss: 0.3123816251754761; Student loss: 0.6795762181282043; Epsilon: 1337.87158203125, r_sum: 39.0\n",
      "Step 268, Generator loss: 0.3183397650718689; Student loss: 0.8411622047424316; Epsilon: 1349.58837890625, r_sum: 28.0\n",
      "Step 269, Generator loss: 0.31948602199554443; Student loss: 0.7771347761154175; Epsilon: 1364.7498779296875, r_sum: 33.0\n",
      "Step 270, Generator loss: 0.311056911945343; Student loss: 0.6761351823806763; Epsilon: 1374.45751953125, r_sum: 39.0\n",
      "Step 271, Generator loss: 0.31673872470855713; Student loss: 0.656267523765564; Epsilon: 1385.633544921875, r_sum: 41.0\n",
      "Step 272, Generator loss: 0.31465470790863037; Student loss: 0.7015269994735718; Epsilon: 1397.0810546875, r_sum: 37.0\n",
      "Step 273, Generator loss: 0.32628804445266724; Student loss: 0.6592586040496826; Epsilon: 1405.106201171875, r_sum: 40.0\n",
      "Step 274, Generator loss: 0.32676270604133606; Student loss: 0.6707682013511658; Epsilon: 1417.9149169921875, r_sum: 40.0\n",
      "Step 275, Generator loss: 0.3268604874610901; Student loss: 0.735567569732666; Epsilon: 1430.941162109375, r_sum: 36.0\n",
      "Step 276, Generator loss: 0.32632970809936523; Student loss: 0.8274843692779541; Epsilon: 1448.13427734375, r_sum: 30.0\n",
      "Step 277, Generator loss: 0.33020254969596863; Student loss: 0.7525181770324707; Epsilon: 1460.0872802734375, r_sum: 33.0\n",
      "Step 278, Generator loss: 0.32597965002059937; Student loss: 0.7822544574737549; Epsilon: 1471.0404052734375, r_sum: 31.0\n",
      "Step 279, Generator loss: 0.335629403591156; Student loss: 0.7468240857124329; Epsilon: 1489.556640625, r_sum: 35.0\n",
      "Step 280, Generator loss: 0.3309829533100128; Student loss: 0.6725166440010071; Epsilon: 1506.4541015625, r_sum: 39.0\n",
      "Step 281, Generator loss: 0.339609295129776; Student loss: 0.8782979249954224; Epsilon: 1519.9207763671875, r_sum: 24.0\n",
      "Step 282, Generator loss: 0.3240017890930176; Student loss: 0.848689079284668; Epsilon: 1537.429931640625, r_sum: 28.0\n",
      "Step 283, Generator loss: 0.3359469175338745; Student loss: 0.7503277063369751; Epsilon: 1546.635009765625, r_sum: 33.0\n",
      "Step 284, Generator loss: 0.3355723023414612; Student loss: 0.7743473052978516; Epsilon: 1561.68994140625, r_sum: 33.0\n",
      "Step 285, Generator loss: 0.3395393490791321; Student loss: 0.7404646277427673; Epsilon: 1574.2984619140625, r_sum: 36.0\n",
      "Step 286, Generator loss: 0.3474065661430359; Student loss: 0.7477526664733887; Epsilon: 1583.9957275390625, r_sum: 33.0\n",
      "Step 287, Generator loss: 0.33721354603767395; Student loss: 0.8720599412918091; Epsilon: 1606.6107177734375, r_sum: 26.0\n",
      "Step 288, Generator loss: 0.3497629165649414; Student loss: 0.7757084369659424; Epsilon: 1621.799560546875, r_sum: 31.0\n",
      "Step 289, Generator loss: 0.342515766620636; Student loss: 0.8429890275001526; Epsilon: 1634.5634765625, r_sum: 27.0\n",
      "Step 290, Generator loss: 0.3440437614917755; Student loss: 0.7979125380516052; Epsilon: 1647.7447509765625, r_sum: 30.0\n",
      "Step 291, Generator loss: 0.3425925672054291; Student loss: 0.740237295627594; Epsilon: 1657.714111328125, r_sum: 33.0\n",
      "Step 292, Generator loss: 0.3541741967201233; Student loss: 0.8434585332870483; Epsilon: 1665.9512939453125, r_sum: 28.0\n",
      "Step 293, Generator loss: 0.3553623557090759; Student loss: 0.7542105913162231; Epsilon: 1680.0076904296875, r_sum: 33.0\n",
      "Step 294, Generator loss: 0.3552786707878113; Student loss: 0.8431691527366638; Epsilon: 1694.6787109375, r_sum: 26.0\n",
      "Step 295, Generator loss: 0.3555900454521179; Student loss: 0.8028476238250732; Epsilon: 1705.772705078125, r_sum: 28.0\n",
      "Step 296, Generator loss: 0.34179866313934326; Student loss: 0.8729544878005981; Epsilon: 1717.18408203125, r_sum: 25.0\n",
      "Step 297, Generator loss: 0.3645798861980438; Student loss: 0.8240454792976379; Epsilon: 1728.71142578125, r_sum: 25.0\n",
      "Step 298, Generator loss: 0.36373889446258545; Student loss: 0.8919873237609863; Epsilon: 1747.00390625, r_sum: 22.0\n",
      "Step 299, Generator loss: 0.3675786554813385; Student loss: 0.8478010892868042; Epsilon: 1759.80419921875, r_sum: 25.0\n",
      "Step 300, Generator loss: 0.3516572117805481; Student loss: 0.8665263652801514; Epsilon: 1774.9083251953125, r_sum: 21.0\n",
      "Step 301, Generator loss: 0.3763203024864197; Student loss: 0.8297908306121826; Epsilon: 1787.472412109375, r_sum: 26.0\n",
      "Step 302, Generator loss: 0.37367284297943115; Student loss: 0.8458898067474365; Epsilon: 1802.082275390625, r_sum: 24.0\n",
      "Step 303, Generator loss: 0.37013620138168335; Student loss: 0.7797880172729492; Epsilon: 1817.7738037109375, r_sum: 28.0\n",
      "Step 304, Generator loss: 0.3877984285354614; Student loss: 0.7785987854003906; Epsilon: 1826.27685546875, r_sum: 29.0\n",
      "Step 305, Generator loss: 0.38136807084083557; Student loss: 0.8999989032745361; Epsilon: 1837.2584228515625, r_sum: 21.0\n",
      "Step 306, Generator loss: 0.38399970531463623; Student loss: 0.8978159427642822; Epsilon: 1848.0526123046875, r_sum: 18.0\n",
      "Step 307, Generator loss: 0.3826226592063904; Student loss: 0.958503782749176; Epsilon: 1861.656494140625, r_sum: 15.0\n",
      "Step 308, Generator loss: 0.3847428262233734; Student loss: 0.7976876497268677; Epsilon: 1875.735595703125, r_sum: 28.0\n",
      "Step 309, Generator loss: 0.37902307510375977; Student loss: 0.8895373344421387; Epsilon: 1888.8931884765625, r_sum: 19.0\n",
      "Step 310, Generator loss: 0.3996742367744446; Student loss: 0.747504711151123; Epsilon: 1904.0933837890625, r_sum: 31.0\n",
      "Step 311, Generator loss: 0.39075303077697754; Student loss: 0.8719992637634277; Epsilon: 1914.8453369140625, r_sum: 22.0\n",
      "Step 312, Generator loss: 0.3971743583679199; Student loss: 0.8233228921890259; Epsilon: 1933.47900390625, r_sum: 25.0\n",
      "Step 313, Generator loss: 0.4017206132411957; Student loss: 0.8730238080024719; Epsilon: 1945.536865234375, r_sum: 21.0\n",
      "Step 314, Generator loss: 0.40414127707481384; Student loss: 0.9211887121200562; Epsilon: 1956.16015625, r_sum: 16.0\n",
      "Step 315, Generator loss: 0.40181764960289; Student loss: 0.8128098845481873; Epsilon: 1969.321533203125, r_sum: 24.0\n",
      "Step 316, Generator loss: 0.4197051227092743; Student loss: 0.9170867204666138; Epsilon: 1976.9713134765625, r_sum: 19.0\n",
      "Step 317, Generator loss: 0.41831159591674805; Student loss: 0.8987355828285217; Epsilon: 1988.92041015625, r_sum: 17.0\n",
      "Step 318, Generator loss: 0.404604971408844; Student loss: 0.8460367918014526; Epsilon: 1999.449951171875, r_sum: 21.0\n",
      "Step 319, Generator loss: 0.4202108383178711; Student loss: 0.7987431287765503; Epsilon: 2008.4127197265625, r_sum: 25.0\n",
      "Step 320, Generator loss: 0.423425555229187; Student loss: 0.764653742313385; Epsilon: 2024.70751953125, r_sum: 27.0\n",
      "Step 321, Generator loss: 0.43628931045532227; Student loss: 0.9318585395812988; Epsilon: 2037.9268798828125, r_sum: 12.0\n",
      "Step 322, Generator loss: 0.42193520069122314; Student loss: 0.8939101696014404; Epsilon: 2048.870361328125, r_sum: 14.0\n",
      "Step 323, Generator loss: 0.4271083474159241; Student loss: 0.8521592020988464; Epsilon: 2062.708740234375, r_sum: 21.0\n",
      "Step 324, Generator loss: 0.4251479208469391; Student loss: 0.8946453332901001; Epsilon: 2078.32373046875, r_sum: 14.0\n",
      "Step 325, Generator loss: 0.4367561936378479; Student loss: 0.8149476051330566; Epsilon: 2098.0791015625, r_sum: 23.0\n",
      "Step 326, Generator loss: 0.44369369745254517; Student loss: 0.8812163472175598; Epsilon: 2107.898681640625, r_sum: 17.0\n",
      "Step 327, Generator loss: 0.44667553901672363; Student loss: 0.8743768930435181; Epsilon: 2119.22265625, r_sum: 17.0\n",
      "Step 328, Generator loss: 0.44214609265327454; Student loss: 0.8495975732803345; Epsilon: 2130.392822265625, r_sum: 18.0\n",
      "Step 329, Generator loss: 0.43983420729637146; Student loss: 0.8710648417472839; Epsilon: 2140.28759765625, r_sum: 17.0\n",
      "Step 330, Generator loss: 0.44355344772338867; Student loss: 0.8738994598388672; Epsilon: 2151.421875, r_sum: 17.0\n",
      "Step 331, Generator loss: 0.45809200406074524; Student loss: 0.8213409185409546; Epsilon: 2157.670166015625, r_sum: 19.0\n",
      "Step 332, Generator loss: 0.45536544919013977; Student loss: 0.8829345703125; Epsilon: 2164.243408203125, r_sum: 15.0\n",
      "Step 333, Generator loss: 0.46026530861854553; Student loss: 0.8378046154975891; Epsilon: 2180.04541015625, r_sum: 19.0\n",
      "Step 334, Generator loss: 0.471604585647583; Student loss: 0.8287249803543091; Epsilon: 2188.34716796875, r_sum: 18.0\n",
      "Step 335, Generator loss: 0.4551142752170563; Student loss: 0.8487436771392822; Epsilon: 2197.02197265625, r_sum: 17.0\n",
      "Step 336, Generator loss: 0.480449378490448; Student loss: 0.8711181879043579; Epsilon: 2210.821044921875, r_sum: 15.0\n",
      "Step 337, Generator loss: 0.465537428855896; Student loss: 0.8929868936538696; Epsilon: 2222.08984375, r_sum: 12.0\n",
      "Step 338, Generator loss: 0.462056040763855; Student loss: 0.8876971006393433; Epsilon: 2231.16650390625, r_sum: 13.0\n",
      "Step 339, Generator loss: 0.4679070711135864; Student loss: 0.8077412247657776; Epsilon: 2240.583251953125, r_sum: 19.0\n",
      "Step 340, Generator loss: 0.48176002502441406; Student loss: 0.8388609290122986; Epsilon: 2247.58154296875, r_sum: 17.0\n",
      "Step 341, Generator loss: 0.4758552014827728; Student loss: 0.816703200340271; Epsilon: 2257.647705078125, r_sum: 20.0\n",
      "Step 342, Generator loss: 0.48360174894332886; Student loss: 0.9332770109176636; Epsilon: 2266.30224609375, r_sum: 8.0\n",
      "Step 343, Generator loss: 0.4882901906967163; Student loss: 0.8442972302436829; Epsilon: 2275.69189453125, r_sum: 14.0\n",
      "Step 344, Generator loss: 0.5014010667800903; Student loss: 0.8229812383651733; Epsilon: 2289.55517578125, r_sum: 16.0\n",
      "Step 345, Generator loss: 0.49323946237564087; Student loss: 0.9092530608177185; Epsilon: 2300.298828125, r_sum: 7.0\n",
      "Step 346, Generator loss: 0.5112139582633972; Student loss: 0.852537214756012; Epsilon: 2306.44384765625, r_sum: 11.0\n",
      "Step 347, Generator loss: 0.5100308656692505; Student loss: 0.8639118075370789; Epsilon: 2319.538818359375, r_sum: 11.0\n",
      "Step 348, Generator loss: 0.49556899070739746; Student loss: 0.8127868175506592; Epsilon: 2327.956298828125, r_sum: 16.0\n",
      "Step 349, Generator loss: 0.5033110976219177; Student loss: 0.8278318047523499; Epsilon: 2338.048095703125, r_sum: 10.0\n",
      "Step 350, Generator loss: 0.5130102634429932; Student loss: 0.8046636581420898; Epsilon: 2349.533447265625, r_sum: 15.0\n",
      "Step 351, Generator loss: 0.518969714641571; Student loss: 0.843734622001648; Epsilon: 2360.185302734375, r_sum: 11.0\n",
      "Step 352, Generator loss: 0.5137655735015869; Student loss: 0.8185992240905762; Epsilon: 2366.5302734375, r_sum: 12.0\n",
      "Step 353, Generator loss: 0.4920728802680969; Student loss: 0.8468040227890015; Epsilon: 2375.41357421875, r_sum: 7.0\n",
      "Step 354, Generator loss: 0.5315205454826355; Student loss: 0.8378775715827942; Epsilon: 2390.37353515625, r_sum: 11.0\n",
      "Step 355, Generator loss: 0.5332802534103394; Student loss: 0.8007414937019348; Epsilon: 2397.902099609375, r_sum: 15.0\n",
      "Step 356, Generator loss: 0.5584505796432495; Student loss: 0.8026893138885498; Epsilon: 2407.646484375, r_sum: 12.0\n",
      "Step 357, Generator loss: 0.5477197766304016; Student loss: 0.7952170372009277; Epsilon: 2414.58984375, r_sum: 14.0\n",
      "Step 358, Generator loss: 0.5378351211547852; Student loss: 0.8251863718032837; Epsilon: 2423.743408203125, r_sum: 12.0\n",
      "Step 359, Generator loss: 0.5494325160980225; Student loss: 0.8167113065719604; Epsilon: 2428.294189453125, r_sum: 10.0\n",
      "Step 360, Generator loss: 0.5634211897850037; Student loss: 0.8217041492462158; Epsilon: 2440.321533203125, r_sum: 12.0\n",
      "Step 361, Generator loss: 0.5452995300292969; Student loss: 0.7842327952384949; Epsilon: 2447.197998046875, r_sum: 15.0\n",
      "Step 362, Generator loss: 0.5648593902587891; Student loss: 0.7970149517059326; Epsilon: 2453.175048828125, r_sum: 10.0\n",
      "Step 363, Generator loss: 0.5508015751838684; Student loss: 0.7908480167388916; Epsilon: 2462.2060546875, r_sum: 12.0\n",
      "Step 364, Generator loss: 0.5540344715118408; Student loss: 0.8109461069107056; Epsilon: 2467.49853515625, r_sum: 8.0\n",
      "Step 365, Generator loss: 0.5629987716674805; Student loss: 0.8268522024154663; Epsilon: 2478.55224609375, r_sum: 5.0\n",
      "Step 366, Generator loss: 0.5657268762588501; Student loss: 0.8057793378829956; Epsilon: 2484.655517578125, r_sum: 7.0\n",
      "Step 367, Generator loss: 0.5742906332015991; Student loss: 0.7720828056335449; Epsilon: 2492.9541015625, r_sum: 14.0\n",
      "Step 368, Generator loss: 0.5543742179870605; Student loss: 0.7954807281494141; Epsilon: 2495.550048828125, r_sum: 8.0\n",
      "Step 369, Generator loss: 0.5658527612686157; Student loss: 0.776594877243042; Epsilon: 2498.548583984375, r_sum: 10.0\n",
      "Step 370, Generator loss: 0.5768592357635498; Student loss: 0.7984175086021423; Epsilon: 2502.777099609375, r_sum: 10.0\n",
      "Step 371, Generator loss: 0.5927227735519409; Student loss: 0.7919917106628418; Epsilon: 2506.76171875, r_sum: 9.0\n",
      "Step 372, Generator loss: 0.5979921221733093; Student loss: 0.8051032423973083; Epsilon: 2510.735595703125, r_sum: 4.0\n",
      "Step 373, Generator loss: 0.5852887630462646; Student loss: 0.7805596590042114; Epsilon: 2518.372802734375, r_sum: 10.0\n",
      "Step 374, Generator loss: 0.5985060930252075; Student loss: 0.7775117754936218; Epsilon: 2520.1875, r_sum: 6.0\n",
      "Step 375, Generator loss: 0.5890951156616211; Student loss: 0.778209924697876; Epsilon: 2525.77587890625, r_sum: 7.0\n",
      "Step 376, Generator loss: 0.6013283729553223; Student loss: 0.7797987461090088; Epsilon: 2531.1201171875, r_sum: 7.0\n",
      "Step 377, Generator loss: 0.6090248823165894; Student loss: 0.7638517022132874; Epsilon: 2538.37646484375, r_sum: 9.0\n",
      "Step 378, Generator loss: 0.6197903752326965; Student loss: 0.761479377746582; Epsilon: 2546.50146484375, r_sum: 9.0\n",
      "Step 379, Generator loss: 0.592585563659668; Student loss: 0.7785193920135498; Epsilon: 2551.747802734375, r_sum: 7.0\n",
      "Step 380, Generator loss: 0.604590654373169; Student loss: 0.7668290138244629; Epsilon: 2555.634765625, r_sum: 9.0\n",
      "Step 381, Generator loss: 0.6102609038352966; Student loss: 0.7687889337539673; Epsilon: 2560.12109375, r_sum: 5.0\n",
      "Step 382, Generator loss: 0.6115943193435669; Student loss: 0.7337663173675537; Epsilon: 2567.907470703125, r_sum: 6.0\n",
      "Step 383, Generator loss: 0.632582426071167; Student loss: 0.7678283452987671; Epsilon: 2570.84130859375, r_sum: 5.0\n",
      "Step 384, Generator loss: 0.6455769538879395; Student loss: 0.7384214997291565; Epsilon: 2575.73779296875, r_sum: 6.0\n",
      "Step 385, Generator loss: 0.6545090675354004; Student loss: 0.7470287084579468; Epsilon: 2581.8046875, r_sum: 4.0\n",
      "Step 386, Generator loss: 0.6402334570884705; Student loss: 0.7749495506286621; Epsilon: 2585.109130859375, r_sum: 2.0\n",
      "Step 387, Generator loss: 0.635485053062439; Student loss: 0.7699060440063477; Epsilon: 2590.789794921875, r_sum: 3.0\n",
      "Step 388, Generator loss: 0.6513311266899109; Student loss: 0.7574639320373535; Epsilon: 2595.072998046875, r_sum: 3.0\n",
      "Step 389, Generator loss: 0.6414856910705566; Student loss: 0.7488202452659607; Epsilon: 2602.3466796875, r_sum: 4.0\n",
      "Step 390, Generator loss: 0.6522226333618164; Student loss: 0.746057391166687; Epsilon: 2607.400146484375, r_sum: 9.0\n",
      "Step 391, Generator loss: 0.6553220748901367; Student loss: 0.7517275810241699; Epsilon: 2610.185791015625, r_sum: 3.0\n",
      "Step 392, Generator loss: 0.6540563106536865; Student loss: 0.7334108352661133; Epsilon: 2613.483642578125, r_sum: 6.0\n",
      "Step 393, Generator loss: 0.6638040542602539; Student loss: 0.7469968795776367; Epsilon: 2617.947998046875, r_sum: 8.0\n",
      "Step 394, Generator loss: 0.6808886528015137; Student loss: 0.7310503721237183; Epsilon: 2622.84521484375, r_sum: 3.0\n",
      "Step 395, Generator loss: 0.6818391680717468; Student loss: 0.7293740510940552; Epsilon: 2625.507568359375, r_sum: 1.0\n",
      "Step 396, Generator loss: 0.6708579063415527; Student loss: 0.7221169471740723; Epsilon: 2632.415771484375, r_sum: 9.0\n",
      "Step 397, Generator loss: 0.6578062176704407; Student loss: 0.7168006896972656; Epsilon: 2632.27294921875, r_sum: 1.0\n",
      "Step 398, Generator loss: 0.7036486864089966; Student loss: 0.7039385437965393; Epsilon: 2635.512451171875, r_sum: 4.0\n",
      "Step 399, Generator loss: 0.6889389753341675; Student loss: 0.7073401212692261; Epsilon: 2640.202392578125, r_sum: 4.0\n",
      "Step 400, Generator loss: 0.6769371032714844; Student loss: 0.6992164850234985; Epsilon: 2642.2509765625, r_sum: 3.0\n",
      "Step 401, Generator loss: 0.7130076289176941; Student loss: 0.699384331703186; Epsilon: 2644.10107421875, r_sum: 8.0\n",
      "Step 402, Generator loss: 0.7345002293586731; Student loss: 0.7379823923110962; Epsilon: 2649.53125, r_sum: 7.0\n",
      "Step 403, Generator loss: 0.7164739370346069; Student loss: 0.6758118867874146; Epsilon: 2652.75634765625, r_sum: 5.0\n",
      "Step 404, Generator loss: 0.7109490633010864; Student loss: 0.680622935295105; Epsilon: 2655.267578125, r_sum: 5.0\n",
      "Step 405, Generator loss: 0.6804167032241821; Student loss: 0.7005122900009155; Epsilon: 2658.342041015625, r_sum: 3.0\n",
      "Step 406, Generator loss: 0.7107272744178772; Student loss: 0.6747838258743286; Epsilon: 2658.585693359375, r_sum: 1.0\n",
      "Step 407, Generator loss: 0.7279975414276123; Student loss: 0.6795223951339722; Epsilon: 2659.865478515625, r_sum: 2.0\n",
      "Step 408, Generator loss: 0.7083840370178223; Student loss: 0.6828867197036743; Epsilon: 2664.444091796875, r_sum: 2.0\n",
      "Step 409, Generator loss: 0.7189143896102905; Student loss: 0.6668121814727783; Epsilon: 2666.974365234375, r_sum: 1.0\n",
      "Step 410, Generator loss: 0.7193610668182373; Student loss: 0.6921712160110474; Epsilon: 2671.441650390625, r_sum: 7.0\n",
      "Step 411, Generator loss: 0.7171393036842346; Student loss: 0.6702239513397217; Epsilon: 2673.019775390625, r_sum: 2.0\n",
      "Step 412, Generator loss: 0.7226870656013489; Student loss: 0.6920802593231201; Epsilon: 2675.910888671875, r_sum: 2.0\n",
      "Step 413, Generator loss: 0.7543187737464905; Student loss: 0.6829498410224915; Epsilon: 2677.451171875, r_sum: 1.0\n",
      "Step 414, Generator loss: 0.725368857383728; Student loss: 0.6608209013938904; Epsilon: 2680.9921875, r_sum: 4.0\n",
      "Step 415, Generator loss: 0.7693017721176147; Student loss: 0.6472076177597046; Epsilon: 2684.967529296875, r_sum: 5.0\n",
      "Step 416, Generator loss: 0.722775936126709; Student loss: 0.6508908867835999; Epsilon: 2689.658935546875, r_sum: 2.0\n",
      "Step 417, Generator loss: 0.7411818504333496; Student loss: 0.6552032828330994; Epsilon: 2691.390869140625, r_sum: 3.0\n",
      "Step 418, Generator loss: 0.759179949760437; Student loss: 0.6406618356704712; Epsilon: 2693.0712890625, r_sum: 2.0\n",
      "Step 419, Generator loss: 0.8026330471038818; Student loss: 0.649675726890564; Epsilon: 2696.888427734375, r_sum: 6.0\n",
      "Step 420, Generator loss: 0.7329607009887695; Student loss: 0.6330236196517944; Epsilon: 2697.588623046875, r_sum: 1.0\n",
      "Step 421, Generator loss: 0.7733967304229736; Student loss: 0.617093026638031; Epsilon: 2698.070556640625, r_sum: 2.0\n",
      "Step 422, Generator loss: 0.7584341764450073; Student loss: 0.5980803966522217; Epsilon: 2699.4482421875, r_sum: 3.0\n",
      "Step 423, Generator loss: 0.7968270182609558; Student loss: 0.6173123717308044; Epsilon: 2701.827392578125, r_sum: 2.0\n",
      "Step 424, Generator loss: 0.7817938327789307; Student loss: 0.6200990080833435; Epsilon: 2704.182861328125, r_sum: 2.0\n",
      "Step 425, Generator loss: 0.7935472726821899; Student loss: 0.629096269607544; Epsilon: 2708.39501953125, r_sum: 2.0\n",
      "Step 426, Generator loss: 0.7911669611930847; Student loss: 0.6205064654350281; Epsilon: 2709.735107421875, r_sum: 0.0\n",
      "Step 427, Generator loss: 0.7987610101699829; Student loss: 0.6117901802062988; Epsilon: 2712.966552734375, r_sum: 1.0\n",
      "Step 428, Generator loss: 0.8047605752944946; Student loss: 0.6185762882232666; Epsilon: 2715.726806640625, r_sum: 1.0\n",
      "Step 429, Generator loss: 0.8326147794723511; Student loss: 0.6631619930267334; Epsilon: 2715.879638671875, r_sum: 3.0\n",
      "Step 430, Generator loss: 0.8197044134140015; Student loss: 0.6131261587142944; Epsilon: 2717.05419921875, r_sum: 1.0\n",
      "Step 431, Generator loss: 0.8330435752868652; Student loss: 0.5842262506484985; Epsilon: 2717.865234375, r_sum: 0.0\n",
      "Step 432, Generator loss: 0.8303239941596985; Student loss: 0.5868737697601318; Epsilon: 2718.755126953125, r_sum: 1.0\n",
      "Step 433, Generator loss: 0.8064351081848145; Student loss: 0.5834615230560303; Epsilon: 2720.59033203125, r_sum: 1.0\n",
      "Step 434, Generator loss: 0.8234711289405823; Student loss: 0.5828025937080383; Epsilon: 2722.610595703125, r_sum: 3.0\n",
      "Step 435, Generator loss: 0.8046897053718567; Student loss: 0.5840540528297424; Epsilon: 2723.788330078125, r_sum: 0.0\n",
      "Step 436, Generator loss: 0.8343508243560791; Student loss: 0.5917859077453613; Epsilon: 2727.203125, r_sum: 4.0\n",
      "Step 437, Generator loss: 0.8434538245201111; Student loss: 0.588802695274353; Epsilon: 2728.8408203125, r_sum: 2.0\n",
      "Step 438, Generator loss: 0.8418375253677368; Student loss: 0.5639524459838867; Epsilon: 2730.77490234375, r_sum: 1.0\n",
      "Step 439, Generator loss: 0.8153330087661743; Student loss: 0.5607948899269104; Epsilon: 2731.928955078125, r_sum: 0.0\n",
      "Step 440, Generator loss: 0.8357901573181152; Student loss: 0.5895462036132812; Epsilon: 2732.7548828125, r_sum: 1.0\n",
      "Step 441, Generator loss: 0.8446800708770752; Student loss: 0.5689208507537842; Epsilon: 2733.9462890625, r_sum: 1.0\n",
      "Step 442, Generator loss: 0.8540785908699036; Student loss: 0.559093177318573; Epsilon: 2736.07080078125, r_sum: 3.0\n",
      "Step 443, Generator loss: 0.8886768817901611; Student loss: 0.5673326253890991; Epsilon: 2736.392578125, r_sum: 1.0\n",
      "Step 444, Generator loss: 0.8524707555770874; Student loss: 0.57471764087677; Epsilon: 2737.229736328125, r_sum: 3.0\n",
      "Step 445, Generator loss: 0.8684393763542175; Student loss: 0.5688315033912659; Epsilon: 2739.103515625, r_sum: 4.0\n",
      "Step 446, Generator loss: 0.8922266960144043; Student loss: 0.5465344190597534; Epsilon: 2741.114013671875, r_sum: 0.0\n",
      "Step 447, Generator loss: 0.8864717483520508; Student loss: 0.539922833442688; Epsilon: 2741.330322265625, r_sum: 0.0\n",
      "Step 448, Generator loss: 0.8792325854301453; Student loss: 0.5484755635261536; Epsilon: 2742.91943359375, r_sum: 1.0\n",
      "Step 449, Generator loss: 0.8983231782913208; Student loss: 0.5430803298950195; Epsilon: 2747.6376953125, r_sum: 2.0\n",
      "Step 450, Generator loss: 0.8778293132781982; Student loss: 0.5595089793205261; Epsilon: 2748.260498046875, r_sum: 1.0\n",
      "Step 451, Generator loss: 0.9080848693847656; Student loss: 0.520186185836792; Epsilon: 2748.334716796875, r_sum: 1.0\n",
      "Step 452, Generator loss: 0.8899828195571899; Student loss: 0.552409827709198; Epsilon: 2748.9140625, r_sum: 1.0\n",
      "Step 453, Generator loss: 0.8961337208747864; Student loss: 0.5410929322242737; Epsilon: 2748.942626953125, r_sum: 0.0\n",
      "Step 454, Generator loss: 0.916992723941803; Student loss: 0.5456037521362305; Epsilon: 2750.15771484375, r_sum: 1.0\n",
      "Step 455, Generator loss: 0.9218364953994751; Student loss: 0.528928279876709; Epsilon: 2750.68603515625, r_sum: 1.0\n",
      "Step 456, Generator loss: 0.8805555701255798; Student loss: 0.5173001885414124; Epsilon: 2752.261474609375, r_sum: 0.0\n",
      "Step 457, Generator loss: 0.9185426235198975; Student loss: 0.5437871217727661; Epsilon: 2752.982421875, r_sum: 2.0\n",
      "Step 458, Generator loss: 0.9275108575820923; Student loss: 0.5535749197006226; Epsilon: 2754.595458984375, r_sum: 2.0\n",
      "Step 459, Generator loss: 0.9354550838470459; Student loss: 0.5112204551696777; Epsilon: 2757.033447265625, r_sum: 0.0\n",
      "Step 460, Generator loss: 0.9171218872070312; Student loss: 0.5223451852798462; Epsilon: 2757.344970703125, r_sum: 1.0\n",
      "Step 461, Generator loss: 0.9537006616592407; Student loss: 0.5261150002479553; Epsilon: 2757.40380859375, r_sum: 0.0\n",
      "Step 462, Generator loss: 0.8958972096443176; Student loss: 0.5527021288871765; Epsilon: 2758.94189453125, r_sum: 2.0\n",
      "Step 463, Generator loss: 0.953766942024231; Student loss: 0.4909505844116211; Epsilon: 2758.956787109375, r_sum: 0.0\n",
      "Step 464, Generator loss: 0.9664015769958496; Student loss: 0.49640828371047974; Epsilon: 2759.556396484375, r_sum: 0.0\n",
      "Step 465, Generator loss: 0.9242175817489624; Student loss: 0.5111595392227173; Epsilon: 2759.78662109375, r_sum: 0.0\n",
      "Step 466, Generator loss: 0.9432222843170166; Student loss: 0.5169401168823242; Epsilon: 2759.89697265625, r_sum: 0.0\n",
      "Step 467, Generator loss: 0.9451771974563599; Student loss: 0.5087528228759766; Epsilon: 2760.231689453125, r_sum: 0.0\n",
      "Step 468, Generator loss: 0.9409735798835754; Student loss: 0.47570666670799255; Epsilon: 2760.285888671875, r_sum: 0.0\n",
      "Step 469, Generator loss: 0.9929686784744263; Student loss: 0.5005754828453064; Epsilon: 2761.003173828125, r_sum: 1.0\n",
      "Step 470, Generator loss: 0.9547734260559082; Student loss: 0.4899783730506897; Epsilon: 2761.497314453125, r_sum: 0.0\n",
      "Step 471, Generator loss: 0.9821823835372925; Student loss: 0.4939603805541992; Epsilon: 2762.032470703125, r_sum: 0.0\n",
      "Step 472, Generator loss: 0.9416048526763916; Student loss: 0.5005117654800415; Epsilon: 2762.097412109375, r_sum: 0.0\n",
      "Step 473, Generator loss: 0.9581300020217896; Student loss: 0.4964696168899536; Epsilon: 2762.67626953125, r_sum: 2.0\n",
      "Step 474, Generator loss: 0.981515645980835; Student loss: 0.4667774438858032; Epsilon: 2762.810302734375, r_sum: 0.0\n",
      "Step 475, Generator loss: 0.9738009572029114; Student loss: 0.5021525621414185; Epsilon: 2762.666748046875, r_sum: 1.0\n",
      "Step 476, Generator loss: 0.947496771812439; Student loss: 0.4911370575428009; Epsilon: 2762.771728515625, r_sum: 0.0\n",
      "Step 477, Generator loss: 0.9635343551635742; Student loss: 0.503544270992279; Epsilon: 2763.9677734375, r_sum: 0.0\n",
      "Step 478, Generator loss: 0.9938802123069763; Student loss: 0.4877438247203827; Epsilon: 2764.335693359375, r_sum: 1.0\n",
      "Step 479, Generator loss: 0.9893591403961182; Student loss: 0.49062860012054443; Epsilon: 2764.4482421875, r_sum: 2.0\n",
      "Step 480, Generator loss: 0.9826321005821228; Student loss: 0.4758762717247009; Epsilon: 2764.5205078125, r_sum: 0.0\n",
      "Step 481, Generator loss: 0.975974440574646; Student loss: 0.508177638053894; Epsilon: 2765.700439453125, r_sum: 0.0\n",
      "Step 482, Generator loss: 1.0118783712387085; Student loss: 0.48179763555526733; Epsilon: 2765.8759765625, r_sum: 0.0\n",
      "Step 483, Generator loss: 0.9782180786132812; Student loss: 0.4608229100704193; Epsilon: 2768.68603515625, r_sum: 1.0\n",
      "Step 484, Generator loss: 1.0047283172607422; Student loss: 0.4472319781780243; Epsilon: 2768.417236328125, r_sum: 1.0\n",
      "Step 485, Generator loss: 1.002827763557434; Student loss: 0.48377737402915955; Epsilon: 2768.64306640625, r_sum: 0.0\n",
      "Step 486, Generator loss: 1.0002788305282593; Student loss: 0.4771064519882202; Epsilon: 2768.714111328125, r_sum: 0.0\n",
      "Step 487, Generator loss: 1.0216647386550903; Student loss: 0.44868335127830505; Epsilon: 2769.774169921875, r_sum: 1.0\n",
      "Step 488, Generator loss: 1.0216182470321655; Student loss: 0.46895334124565125; Epsilon: 2769.89208984375, r_sum: 1.0\n",
      "Step 489, Generator loss: 1.0093175172805786; Student loss: 0.4281202554702759; Epsilon: 2769.910888671875, r_sum: 0.0\n",
      "Step 490, Generator loss: 1.002666711807251; Student loss: 0.44116875529289246; Epsilon: 2770.968994140625, r_sum: 0.0\n",
      "Step 491, Generator loss: 1.0558627843856812; Student loss: 0.46584552526474; Epsilon: 2771.081298828125, r_sum: 0.0\n",
      "Step 492, Generator loss: 1.0297660827636719; Student loss: 0.45098280906677246; Epsilon: 2771.243896484375, r_sum: 0.0\n",
      "Step 493, Generator loss: 1.0122718811035156; Student loss: 0.46979275345802307; Epsilon: 2771.710693359375, r_sum: 0.0\n",
      "Step 494, Generator loss: 1.0445339679718018; Student loss: 0.44328486919403076; Epsilon: 2771.7587890625, r_sum: 0.0\n",
      "Step 495, Generator loss: 1.013960361480713; Student loss: 0.4329095482826233; Epsilon: 2771.829833984375, r_sum: 0.0\n",
      "Step 496, Generator loss: 1.1054565906524658; Student loss: 0.42850905656814575; Epsilon: 2771.879638671875, r_sum: 0.0\n",
      "Step 497, Generator loss: 1.0484673976898193; Student loss: 0.4193647503852844; Epsilon: 2771.8935546875, r_sum: 0.0\n",
      "Step 498, Generator loss: 1.0423510074615479; Student loss: 0.45267608761787415; Epsilon: 2772.02099609375, r_sum: 1.0\n",
      "Step 499, Generator loss: 1.1138389110565186; Student loss: 0.42437130212783813; Epsilon: 2772.2822265625, r_sum: 0.0\n",
      "Step 500, Generator loss: 1.029144048690796; Student loss: 0.45706725120544434; Epsilon: 2772.3564453125, r_sum: 0.0\n",
      "Step 501, Generator loss: 1.0606497526168823; Student loss: 0.45319312810897827; Epsilon: 2772.366943359375, r_sum: 0.0\n",
      "Step 502, Generator loss: 1.1001529693603516; Student loss: 0.4577954411506653; Epsilon: 2773.448974609375, r_sum: 1.0\n",
      "Step 503, Generator loss: 1.0444369316101074; Student loss: 0.45631057024002075; Epsilon: 2773.930419921875, r_sum: 1.0\n",
      "Step 504, Generator loss: 1.0859756469726562; Student loss: 0.43996304273605347; Epsilon: 2774.36181640625, r_sum: 1.0\n",
      "Step 505, Generator loss: 1.070312261581421; Student loss: 0.45185279846191406; Epsilon: 2775.431884765625, r_sum: 1.0\n",
      "Step 506, Generator loss: 1.1203742027282715; Student loss: 0.43804579973220825; Epsilon: 2775.55126953125, r_sum: 0.0\n",
      "Step 507, Generator loss: 1.0836048126220703; Student loss: 0.43627995252609253; Epsilon: 2776.20556640625, r_sum: 0.0\n",
      "Step 508, Generator loss: 1.0723798274993896; Student loss: 0.42147722840309143; Epsilon: 2776.62060546875, r_sum: 0.0\n",
      "Step 509, Generator loss: 1.076332449913025; Student loss: 0.4293089509010315; Epsilon: 2776.732421875, r_sum: 1.0\n",
      "Step 510, Generator loss: 1.0973012447357178; Student loss: 0.4520670771598816; Epsilon: 2776.87939453125, r_sum: 0.0\n",
      "Step 511, Generator loss: 1.096313714981079; Student loss: 0.439272940158844; Epsilon: 2776.882080078125, r_sum: 0.0\n",
      "Step 512, Generator loss: 1.1066532135009766; Student loss: 0.4244000315666199; Epsilon: 2776.884765625, r_sum: 0.0\n",
      "Step 513, Generator loss: 1.1112173795700073; Student loss: 0.44009923934936523; Epsilon: 2776.893798828125, r_sum: 0.0\n",
      "Step 514, Generator loss: 1.1532084941864014; Student loss: 0.4185987114906311; Epsilon: 2776.89990234375, r_sum: 0.0\n",
      "Step 515, Generator loss: 1.086404800415039; Student loss: 0.43780970573425293; Epsilon: 2777.043701171875, r_sum: 0.0\n",
      "Step 516, Generator loss: 1.1105875968933105; Student loss: 0.4301774799823761; Epsilon: 2777.0498046875, r_sum: 0.0\n",
      "Step 517, Generator loss: 1.1235771179199219; Student loss: 0.411899209022522; Epsilon: 2777.05224609375, r_sum: 0.0\n",
      "Step 518, Generator loss: 1.123540997505188; Student loss: 0.42590418457984924; Epsilon: 2778.13427734375, r_sum: 1.0\n",
      "Step 519, Generator loss: 1.120866060256958; Student loss: 0.39890462160110474; Epsilon: 2778.191650390625, r_sum: 0.0\n",
      "Step 520, Generator loss: 1.1249570846557617; Student loss: 0.41149625182151794; Epsilon: 2778.2265625, r_sum: 0.0\n",
      "Step 521, Generator loss: 1.0932822227478027; Student loss: 0.4210984706878662; Epsilon: 2778.24560546875, r_sum: 1.0\n",
      "Step 522, Generator loss: 1.089188814163208; Student loss: 0.41694220900535583; Epsilon: 2778.263427734375, r_sum: 1.0\n",
      "Step 523, Generator loss: 1.1062840223312378; Student loss: 0.42923617362976074; Epsilon: 2778.40234375, r_sum: 0.0\n",
      "Step 524, Generator loss: 1.1607012748718262; Student loss: 0.4221108555793762; Epsilon: 2778.4169921875, r_sum: 0.0\n",
      "Step 525, Generator loss: 1.090675711631775; Student loss: 0.38048774003982544; Epsilon: 2778.517333984375, r_sum: 0.0\n",
      "Step 526, Generator loss: 1.1181559562683105; Student loss: 0.4031114876270294; Epsilon: 2778.636474609375, r_sum: 0.0\n",
      "Step 527, Generator loss: 1.111018180847168; Student loss: 0.41454797983169556; Epsilon: 2779.0556640625, r_sum: 0.0\n",
      "Step 528, Generator loss: 1.100360631942749; Student loss: 0.4157816767692566; Epsilon: 2779.067138671875, r_sum: 0.0\n",
      "Step 529, Generator loss: 1.1003451347351074; Student loss: 0.41807109117507935; Epsilon: 2779.077880859375, r_sum: 0.0\n",
      "Step 530, Generator loss: 1.12168550491333; Student loss: 0.40206238627433777; Epsilon: 2779.08837890625, r_sum: 0.0\n",
      "Step 531, Generator loss: 1.116119146347046; Student loss: 0.4158294200897217; Epsilon: 2779.11083984375, r_sum: 1.0\n",
      "Step 532, Generator loss: 1.1491825580596924; Student loss: 0.4117732644081116; Epsilon: 2779.13232421875, r_sum: 0.0\n",
      "Step 533, Generator loss: 1.1254130601882935; Student loss: 0.4067383408546448; Epsilon: 2779.228759765625, r_sum: 0.0\n",
      "Step 534, Generator loss: 1.1160478591918945; Student loss: 0.41662120819091797; Epsilon: 2779.914794921875, r_sum: 0.0\n",
      "Step 535, Generator loss: 1.1439175605773926; Student loss: 0.3937780261039734; Epsilon: 2779.91845703125, r_sum: 0.0\n",
      "Step 536, Generator loss: 1.2130591869354248; Student loss: 0.3844450116157532; Epsilon: 2779.940185546875, r_sum: 0.0\n",
      "Step 537, Generator loss: 1.162462592124939; Student loss: 0.3882233500480652; Epsilon: 2779.942138671875, r_sum: 0.0\n",
      "Step 538, Generator loss: 1.1606144905090332; Student loss: 0.398131787776947; Epsilon: 2779.944580078125, r_sum: 0.0\n",
      "Step 539, Generator loss: 1.1468478441238403; Student loss: 0.3873940110206604; Epsilon: 2779.9453125, r_sum: 0.0\n",
      "Step 540, Generator loss: 1.1388342380523682; Student loss: 0.38348016142845154; Epsilon: 2779.968994140625, r_sum: 0.0\n",
      "Step 541, Generator loss: 1.1722729206085205; Student loss: 0.4207971692085266; Epsilon: 2780.009521484375, r_sum: 0.0\n",
      "Step 542, Generator loss: 1.188226342201233; Student loss: 0.3977511525154114; Epsilon: 2780.02734375, r_sum: 0.0\n",
      "Step 543, Generator loss: 1.1245858669281006; Student loss: 0.40271642804145813; Epsilon: 2780.469482421875, r_sum: 1.0\n",
      "Step 544, Generator loss: 1.1946892738342285; Student loss: 0.38446757197380066; Epsilon: 2780.487060546875, r_sum: 0.0\n",
      "Step 545, Generator loss: 1.1616392135620117; Student loss: 0.3999217748641968; Epsilon: 2780.528076171875, r_sum: 0.0\n",
      "Step 546, Generator loss: 1.1231358051300049; Student loss: 0.39329764246940613; Epsilon: 2780.97216796875, r_sum: 1.0\n",
      "Step 547, Generator loss: 1.1893881559371948; Student loss: 0.37936675548553467; Epsilon: 2780.99169921875, r_sum: 0.0\n",
      "Step 548, Generator loss: 1.2068655490875244; Student loss: 0.40814295411109924; Epsilon: 2781.009521484375, r_sum: 1.0\n",
      "Step 549, Generator loss: 1.1327073574066162; Student loss: 0.39352959394454956; Epsilon: 2781.010498046875, r_sum: 0.0\n",
      "Step 550, Generator loss: 1.2017951011657715; Student loss: 0.3916051983833313; Epsilon: 2781.0146484375, r_sum: 0.0\n",
      "Step 551, Generator loss: 1.1896045207977295; Student loss: 0.3725721836090088; Epsilon: 2781.095458984375, r_sum: 0.0\n",
      "Step 552, Generator loss: 1.1823087930679321; Student loss: 0.3978351354598999; Epsilon: 2781.105224609375, r_sum: 0.0\n",
      "Step 553, Generator loss: 1.1810765266418457; Student loss: 0.3893747925758362; Epsilon: 2781.108154296875, r_sum: 0.0\n",
      "Step 554, Generator loss: 1.17526113986969; Student loss: 0.38873210549354553; Epsilon: 2781.115234375, r_sum: 0.0\n",
      "Step 555, Generator loss: 1.2031880617141724; Student loss: 0.4036881625652313; Epsilon: 2781.20947265625, r_sum: 0.0\n",
      "Step 556, Generator loss: 1.1857852935791016; Student loss: 0.40197208523750305; Epsilon: 2781.3955078125, r_sum: 0.0\n",
      "Step 557, Generator loss: 1.1763007640838623; Student loss: 0.3788759410381317; Epsilon: 2781.60205078125, r_sum: 0.0\n",
      "Step 558, Generator loss: 1.2308710813522339; Student loss: 0.38969680666923523; Epsilon: 2781.6103515625, r_sum: 0.0\n",
      "Step 559, Generator loss: 1.2204949855804443; Student loss: 0.3960801362991333; Epsilon: 2781.611328125, r_sum: 0.0\n",
      "Step 560, Generator loss: 1.1850974559783936; Student loss: 0.3800133466720581; Epsilon: 2781.613037109375, r_sum: 0.0\n",
      "Step 561, Generator loss: 1.197554588317871; Student loss: 0.3833288252353668; Epsilon: 2781.70703125, r_sum: 0.0\n",
      "Step 562, Generator loss: 1.1688048839569092; Student loss: 0.3968912363052368; Epsilon: 2781.707275390625, r_sum: 0.0\n",
      "Step 563, Generator loss: 1.1907671689987183; Student loss: 0.37737253308296204; Epsilon: 2781.711669921875, r_sum: 0.0\n",
      "Step 564, Generator loss: 1.213687777519226; Student loss: 0.3682006597518921; Epsilon: 2781.714111328125, r_sum: 0.0\n",
      "Step 565, Generator loss: 1.219118595123291; Student loss: 0.36525586247444153; Epsilon: 2781.7216796875, r_sum: 0.0\n",
      "Step 566, Generator loss: 1.211134910583496; Student loss: 0.37141552567481995; Epsilon: 2781.860107421875, r_sum: 0.0\n",
      "Step 567, Generator loss: 1.199204444885254; Student loss: 0.41823554039001465; Epsilon: 2781.869873046875, r_sum: 1.0\n",
      "Step 568, Generator loss: 1.1949383020401; Student loss: 0.37107160687446594; Epsilon: 2781.90478515625, r_sum: 0.0\n",
      "Step 569, Generator loss: 1.2156721353530884; Student loss: 0.3810955286026001; Epsilon: 2781.906005859375, r_sum: 0.0\n",
      "Step 570, Generator loss: 1.220560073852539; Student loss: 0.3607041835784912; Epsilon: 2781.909423828125, r_sum: 0.0\n",
      "Step 571, Generator loss: 1.2381811141967773; Student loss: 0.3672506809234619; Epsilon: 2781.911376953125, r_sum: 0.0\n",
      "Step 572, Generator loss: 1.2540881633758545; Student loss: 0.3717499375343323; Epsilon: 2781.911865234375, r_sum: 0.0\n",
      "Step 573, Generator loss: 1.2045307159423828; Student loss: 0.3845309615135193; Epsilon: 2782.799560546875, r_sum: 1.0\n",
      "Step 574, Generator loss: 1.1710140705108643; Student loss: 0.3643326759338379; Epsilon: 2782.801513671875, r_sum: 0.0\n",
      "Step 575, Generator loss: 1.2030340433120728; Student loss: 0.379146933555603; Epsilon: 2782.801513671875, r_sum: 0.0\n",
      "Step 576, Generator loss: 1.2481939792633057; Student loss: 0.3638852834701538; Epsilon: 2782.89453125, r_sum: 0.0\n",
      "Step 577, Generator loss: 1.2370686531066895; Student loss: 0.3539220988750458; Epsilon: 2782.894775390625, r_sum: 0.0\n",
      "Step 578, Generator loss: 1.237689733505249; Student loss: 0.3635719418525696; Epsilon: 2782.989013671875, r_sum: 0.0\n",
      "Step 579, Generator loss: 1.2391961812973022; Student loss: 0.3620835840702057; Epsilon: 2782.989501953125, r_sum: 0.0\n",
      "Step 580, Generator loss: 1.185694694519043; Student loss: 0.37284260988235474; Epsilon: 2783.106201171875, r_sum: 1.0\n",
      "Step 581, Generator loss: 1.2191404104232788; Student loss: 0.36746853590011597; Epsilon: 2783.19921875, r_sum: 0.0\n",
      "Step 582, Generator loss: 1.2201740741729736; Student loss: 0.35090169310569763; Epsilon: 2783.201416015625, r_sum: 0.0\n",
      "Step 583, Generator loss: 1.2489145994186401; Student loss: 0.3784351348876953; Epsilon: 2783.201416015625, r_sum: 0.0\n",
      "Step 584, Generator loss: 1.2121787071228027; Student loss: 0.3583495318889618; Epsilon: 2783.21630859375, r_sum: 0.0\n",
      "Step 585, Generator loss: 1.2500348091125488; Student loss: 0.35249602794647217; Epsilon: 2783.217529296875, r_sum: 0.0\n",
      "Step 586, Generator loss: 1.2602667808532715; Student loss: 0.35968610644340515; Epsilon: 2783.219970703125, r_sum: 0.0\n",
      "Step 587, Generator loss: 1.24074125289917; Student loss: 0.3603900671005249; Epsilon: 2783.22705078125, r_sum: 0.0\n",
      "Step 588, Generator loss: 1.2383172512054443; Student loss: 0.3661889433860779; Epsilon: 2783.24462890625, r_sum: 0.0\n",
      "Step 589, Generator loss: 1.2164802551269531; Student loss: 0.34050244092941284; Epsilon: 2783.253173828125, r_sum: 0.0\n",
      "Step 590, Generator loss: 1.2560596466064453; Student loss: 0.35136735439300537; Epsilon: 2783.25390625, r_sum: 0.0\n",
      "Step 591, Generator loss: 1.2385793924331665; Student loss: 0.3587494492530823; Epsilon: 2783.2578125, r_sum: 0.0\n",
      "Step 592, Generator loss: 1.2001901865005493; Student loss: 0.3515849709510803; Epsilon: 2783.25830078125, r_sum: 0.0\n",
      "Step 593, Generator loss: 1.2236037254333496; Student loss: 0.3417292833328247; Epsilon: 2783.259765625, r_sum: 0.0\n",
      "Step 594, Generator loss: 1.2397187948226929; Student loss: 0.362082302570343; Epsilon: 2783.26025390625, r_sum: 0.0\n",
      "Step 595, Generator loss: 1.2671799659729004; Student loss: 0.36366599798202515; Epsilon: 2783.267822265625, r_sum: 0.0\n",
      "Step 596, Generator loss: 1.2440176010131836; Student loss: 0.35817405581474304; Epsilon: 2783.2880859375, r_sum: 0.0\n",
      "Step 597, Generator loss: 1.2559254169464111; Student loss: 0.3411499857902527; Epsilon: 2783.291259765625, r_sum: 0.0\n",
      "Step 598, Generator loss: 1.2251205444335938; Student loss: 0.36033594608306885; Epsilon: 2783.2919921875, r_sum: 0.0\n",
      "Step 599, Generator loss: 1.2559094429016113; Student loss: 0.3753426969051361; Epsilon: 2783.733642578125, r_sum: 1.0\n",
      "Step 600, Generator loss: 1.2012512683868408; Student loss: 0.35240620374679565; Epsilon: 2783.740966796875, r_sum: 0.0\n",
      "Step 601, Generator loss: 1.244333267211914; Student loss: 0.3786160945892334; Epsilon: 2783.7421875, r_sum: 0.0\n",
      "Step 602, Generator loss: 1.2681812047958374; Student loss: 0.3447214365005493; Epsilon: 2783.742431640625, r_sum: 0.0\n",
      "Step 603, Generator loss: 1.2593921422958374; Student loss: 0.35397523641586304; Epsilon: 2783.759765625, r_sum: 0.0\n",
      "Step 604, Generator loss: 1.2553856372833252; Student loss: 0.34584009647369385; Epsilon: 2783.765625, r_sum: 0.0\n",
      "Step 605, Generator loss: 1.214969515800476; Student loss: 0.3521038293838501; Epsilon: 2783.765869140625, r_sum: 0.0\n",
      "Step 606, Generator loss: 1.2975924015045166; Student loss: 0.3382418751716614; Epsilon: 2783.7734375, r_sum: 0.0\n",
      "Step 607, Generator loss: 1.2339591979980469; Student loss: 0.36474019289016724; Epsilon: 2783.7734375, r_sum: 0.0\n",
      "Step 608, Generator loss: 1.2705918550491333; Student loss: 0.3495219349861145; Epsilon: 2783.7763671875, r_sum: 0.0\n",
      "Step 609, Generator loss: 1.2202023267745972; Student loss: 0.3385362923145294; Epsilon: 2783.816650390625, r_sum: 0.0\n",
      "Step 610, Generator loss: 1.2386424541473389; Student loss: 0.3523578643798828; Epsilon: 2783.83203125, r_sum: 0.0\n",
      "Step 611, Generator loss: 1.2347934246063232; Student loss: 0.3606984317302704; Epsilon: 2783.83251953125, r_sum: 0.0\n",
      "Step 612, Generator loss: 1.3242437839508057; Student loss: 0.3554379343986511; Epsilon: 2783.8330078125, r_sum: 0.0\n",
      "Step 613, Generator loss: 1.231245756149292; Student loss: 0.33775025606155396; Epsilon: 2783.8359375, r_sum: 0.0\n",
      "Step 614, Generator loss: 1.253225564956665; Student loss: 0.3574618399143219; Epsilon: 2783.8359375, r_sum: 0.0\n",
      "Step 615, Generator loss: 1.2441915273666382; Student loss: 0.3483622968196869; Epsilon: 2783.8359375, r_sum: 0.0\n",
      "Step 616, Generator loss: 1.2441751956939697; Student loss: 0.3623676896095276; Epsilon: 2783.8359375, r_sum: 0.0\n",
      "Step 617, Generator loss: 1.2808910608291626; Student loss: 0.3455457091331482; Epsilon: 2783.8359375, r_sum: 0.0\n",
      "Step 618, Generator loss: 1.2820805311203003; Student loss: 0.3662215769290924; Epsilon: 2783.84228515625, r_sum: 0.0\n",
      "Step 619, Generator loss: 1.232549786567688; Student loss: 0.34544211626052856; Epsilon: 2783.84228515625, r_sum: 0.0\n",
      "Step 620, Generator loss: 1.256421685218811; Student loss: 0.3444644808769226; Epsilon: 2783.84375, r_sum: 0.0\n",
      "Step 621, Generator loss: 1.2016518115997314; Student loss: 0.349780797958374; Epsilon: 2783.84423828125, r_sum: 0.0\n",
      "Step 622, Generator loss: 1.2254374027252197; Student loss: 0.34413406252861023; Epsilon: 2783.8447265625, r_sum: 0.0\n",
      "Step 623, Generator loss: 1.2236013412475586; Student loss: 0.33593714237213135; Epsilon: 2783.8447265625, r_sum: 0.0\n",
      "Step 624, Generator loss: 1.27095627784729; Student loss: 0.3362533450126648; Epsilon: 2783.844970703125, r_sum: 0.0\n",
      "Step 625, Generator loss: 1.2214627265930176; Student loss: 0.35357964038848877; Epsilon: 2783.84619140625, r_sum: 0.0\n",
      "Step 626, Generator loss: 1.274608850479126; Student loss: 0.32456204295158386; Epsilon: 2783.864013671875, r_sum: 0.0\n",
      "Step 627, Generator loss: 1.2481797933578491; Student loss: 0.3415721654891968; Epsilon: 2783.864990234375, r_sum: 0.0\n",
      "Step 628, Generator loss: 1.2319300174713135; Student loss: 0.35412245988845825; Epsilon: 2783.87255859375, r_sum: 0.0\n",
      "Step 629, Generator loss: 1.2523363828659058; Student loss: 0.368516206741333; Epsilon: 2783.8896484375, r_sum: 1.0\n",
      "Step 630, Generator loss: 1.1890755891799927; Student loss: 0.3577280342578888; Epsilon: 2783.890380859375, r_sum: 0.0\n",
      "Step 631, Generator loss: 1.2421085834503174; Student loss: 0.34095633029937744; Epsilon: 2783.8916015625, r_sum: 0.0\n",
      "Step 632, Generator loss: 1.2669963836669922; Student loss: 0.34885674715042114; Epsilon: 2783.8916015625, r_sum: 0.0\n",
      "Step 633, Generator loss: 1.2597578763961792; Student loss: 0.3602410852909088; Epsilon: 2783.8916015625, r_sum: 0.0\n",
      "Step 634, Generator loss: 1.2684950828552246; Student loss: 0.33318203687667847; Epsilon: 2783.8916015625, r_sum: 0.0\n",
      "Step 635, Generator loss: 1.2958667278289795; Student loss: 0.3456769585609436; Epsilon: 2783.901611328125, r_sum: 0.0\n",
      "Step 636, Generator loss: 1.2987357378005981; Student loss: 0.36079537868499756; Epsilon: 2783.901611328125, r_sum: 0.0\n",
      "Step 637, Generator loss: 1.2457504272460938; Student loss: 0.35394155979156494; Epsilon: 2783.901611328125, r_sum: 0.0\n",
      "Step 638, Generator loss: 1.2481435537338257; Student loss: 0.3710007667541504; Epsilon: 2783.901611328125, r_sum: 0.0\n",
      "Step 639, Generator loss: 1.2462884187698364; Student loss: 0.3482639193534851; Epsilon: 2783.901611328125, r_sum: 0.0\n",
      "Step 640, Generator loss: 1.271638035774231; Student loss: 0.346034973859787; Epsilon: 2783.90185546875, r_sum: 0.0\n",
      "Step 641, Generator loss: 1.2886954545974731; Student loss: 0.3434918224811554; Epsilon: 2783.90185546875, r_sum: 0.0\n",
      "Step 642, Generator loss: 1.224522352218628; Student loss: 0.35206353664398193; Epsilon: 2783.903076171875, r_sum: 0.0\n",
      "Step 643, Generator loss: 1.2511922121047974; Student loss: 0.33977487683296204; Epsilon: 2783.910400390625, r_sum: 0.0\n",
      "Step 644, Generator loss: 1.2783948183059692; Student loss: 0.3359527289867401; Epsilon: 2783.91064453125, r_sum: 0.0\n",
      "Step 645, Generator loss: 1.2410707473754883; Student loss: 0.3494625687599182; Epsilon: 2783.910888671875, r_sum: 0.0\n",
      "Step 646, Generator loss: 1.2650210857391357; Student loss: 0.340549498796463; Epsilon: 2783.910888671875, r_sum: 0.0\n",
      "Step 647, Generator loss: 1.256622314453125; Student loss: 0.33636415004730225; Epsilon: 2784.00341796875, r_sum: 0.0\n",
      "Step 648, Generator loss: 1.2143280506134033; Student loss: 0.3705117106437683; Epsilon: 2784.0048828125, r_sum: 0.0\n",
      "Step 649, Generator loss: 1.2628217935562134; Student loss: 0.348743200302124; Epsilon: 2784.0078125, r_sum: 0.0\n",
      "Step 650, Generator loss: 1.2198017835617065; Student loss: 0.35122162103652954; Epsilon: 2784.0078125, r_sum: 0.0\n",
      "Step 651, Generator loss: 1.2579755783081055; Student loss: 0.35645362734794617; Epsilon: 2784.0078125, r_sum: 0.0\n",
      "Step 652, Generator loss: 1.2379851341247559; Student loss: 0.3456926941871643; Epsilon: 2784.0078125, r_sum: 0.0\n",
      "Step 653, Generator loss: 1.3082098960876465; Student loss: 0.3422945737838745; Epsilon: 2784.0107421875, r_sum: 0.0\n",
      "Step 654, Generator loss: 1.292975664138794; Student loss: 0.34119313955307007; Epsilon: 2784.01123046875, r_sum: 0.0\n",
      "Step 655, Generator loss: 1.2463165521621704; Student loss: 0.3631395101547241; Epsilon: 2784.01123046875, r_sum: 0.0\n",
      "Step 656, Generator loss: 1.3003145456314087; Student loss: 0.3529091477394104; Epsilon: 2784.01123046875, r_sum: 0.0\n",
      "Step 657, Generator loss: 1.235563039779663; Student loss: 0.343084454536438; Epsilon: 2784.01123046875, r_sum: 0.0\n",
      "Step 658, Generator loss: 1.2508689165115356; Student loss: 0.3650554418563843; Epsilon: 2784.01171875, r_sum: 0.0\n",
      "Step 659, Generator loss: 1.2580585479736328; Student loss: 0.35012486577033997; Epsilon: 2784.01171875, r_sum: 0.0\n",
      "Step 660, Generator loss: 1.2428278923034668; Student loss: 0.331268846988678; Epsilon: 2784.01171875, r_sum: 0.0\n",
      "Step 661, Generator loss: 1.2576121091842651; Student loss: 0.3472336530685425; Epsilon: 2784.01171875, r_sum: 0.0\n",
      "Step 662, Generator loss: 1.2321090698242188; Student loss: 0.36621418595314026; Epsilon: 2784.01171875, r_sum: 0.0\n",
      "Step 663, Generator loss: 1.2385902404785156; Student loss: 0.362997829914093; Epsilon: 2784.01171875, r_sum: 0.0\n",
      "Step 664, Generator loss: 1.2485325336456299; Student loss: 0.33819127082824707; Epsilon: 2784.01171875, r_sum: 0.0\n",
      "Step 665, Generator loss: 1.253488540649414; Student loss: 0.33427247405052185; Epsilon: 2784.01171875, r_sum: 0.0\n",
      "Step 666, Generator loss: 1.255124568939209; Student loss: 0.35672351717948914; Epsilon: 2784.018798828125, r_sum: 0.0\n",
      "Step 667, Generator loss: 1.2185170650482178; Student loss: 0.3300365209579468; Epsilon: 2784.018798828125, r_sum: 0.0\n",
      "Step 668, Generator loss: 1.2255864143371582; Student loss: 0.3602893352508545; Epsilon: 2784.019287109375, r_sum: 0.0\n",
      "Step 669, Generator loss: 1.1958897113800049; Student loss: 0.36394011974334717; Epsilon: 2784.019287109375, r_sum: 0.0\n",
      "Step 670, Generator loss: 1.2758575677871704; Student loss: 0.3545144498348236; Epsilon: 2784.019287109375, r_sum: 0.0\n",
      "Step 671, Generator loss: 1.2521716356277466; Student loss: 0.353485643863678; Epsilon: 2784.0205078125, r_sum: 0.0\n",
      "Step 672, Generator loss: 1.2679728269577026; Student loss: 0.34403300285339355; Epsilon: 2784.0205078125, r_sum: 0.0\n",
      "Step 673, Generator loss: 1.2533552646636963; Student loss: 0.33976900577545166; Epsilon: 2784.0205078125, r_sum: 0.0\n",
      "Step 674, Generator loss: 1.2472991943359375; Student loss: 0.3491748869419098; Epsilon: 2784.0205078125, r_sum: 0.0\n",
      "Step 675, Generator loss: 1.2237122058868408; Student loss: 0.33894720673561096; Epsilon: 2784.020751953125, r_sum: 0.0\n",
      "Step 676, Generator loss: 1.2392737865447998; Student loss: 0.33677682280540466; Epsilon: 2784.020751953125, r_sum: 0.0\n",
      "Step 677, Generator loss: 1.2345242500305176; Student loss: 0.3616756200790405; Epsilon: 2784.020751953125, r_sum: 0.0\n",
      "Step 678, Generator loss: 1.2254302501678467; Student loss: 0.36364462971687317; Epsilon: 2784.020751953125, r_sum: 0.0\n",
      "Step 679, Generator loss: 1.2268612384796143; Student loss: 0.3426104187965393; Epsilon: 2784.020751953125, r_sum: 0.0\n",
      "Step 680, Generator loss: 1.2583060264587402; Student loss: 0.341715008020401; Epsilon: 2784.024169921875, r_sum: 0.0\n",
      "Step 681, Generator loss: 1.2457728385925293; Student loss: 0.337258905172348; Epsilon: 2784.03125, r_sum: 0.0\n",
      "Step 682, Generator loss: 1.279484748840332; Student loss: 0.3406786322593689; Epsilon: 2784.03173828125, r_sum: 0.0\n",
      "Step 683, Generator loss: 1.263898491859436; Student loss: 0.3589128255844116; Epsilon: 2784.03173828125, r_sum: 0.0\n",
      "Step 684, Generator loss: 1.251625657081604; Student loss: 0.36065495014190674; Epsilon: 2784.03173828125, r_sum: 0.0\n",
      "Step 685, Generator loss: 1.2439628839492798; Student loss: 0.34620410203933716; Epsilon: 2784.03173828125, r_sum: 0.0\n",
      "Step 686, Generator loss: 1.2517657279968262; Student loss: 0.34969037771224976; Epsilon: 2784.03173828125, r_sum: 0.0\n",
      "Step 687, Generator loss: 1.2380468845367432; Student loss: 0.3476371169090271; Epsilon: 2784.03173828125, r_sum: 0.0\n",
      "Step 688, Generator loss: 1.2470390796661377; Student loss: 0.3569124937057495; Epsilon: 2784.03173828125, r_sum: 0.0\n",
      "Step 689, Generator loss: 1.2628402709960938; Student loss: 0.34572833776474; Epsilon: 2784.03173828125, r_sum: 0.0\n",
      "Step 690, Generator loss: 1.2404769659042358; Student loss: 0.34243279695510864; Epsilon: 2784.03173828125, r_sum: 0.0\n",
      "Step 691, Generator loss: 1.2273638248443604; Student loss: 0.36195188760757446; Epsilon: 2784.03173828125, r_sum: 0.0\n",
      "Step 692, Generator loss: 1.2778651714324951; Student loss: 0.343034952878952; Epsilon: 2784.03173828125, r_sum: 0.0\n",
      "Step 693, Generator loss: 1.2917351722717285; Student loss: 0.3499928414821625; Epsilon: 2784.03173828125, r_sum: 0.0\n",
      "Step 694, Generator loss: 1.2435853481292725; Student loss: 0.3479466438293457; Epsilon: 2784.03173828125, r_sum: 0.0\n",
      "Step 695, Generator loss: 1.2473381757736206; Student loss: 0.3374252915382385; Epsilon: 2784.03173828125, r_sum: 0.0\n",
      "Step 696, Generator loss: 1.222449779510498; Student loss: 0.36430925130844116; Epsilon: 2784.042724609375, r_sum: 0.0\n",
      "Step 697, Generator loss: 1.247254729270935; Student loss: 0.35295262932777405; Epsilon: 2784.042724609375, r_sum: 0.0\n",
      "Step 698, Generator loss: 1.2162927389144897; Student loss: 0.34573304653167725; Epsilon: 2784.042724609375, r_sum: 0.0\n",
      "Step 699, Generator loss: 1.2638906240463257; Student loss: 0.3292057514190674; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 700, Generator loss: 1.2399204969406128; Student loss: 0.34855201840400696; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 701, Generator loss: 1.2492454051971436; Student loss: 0.3354277014732361; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 702, Generator loss: 1.2792540788650513; Student loss: 0.35266003012657166; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 703, Generator loss: 1.222092628479004; Student loss: 0.33742567896842957; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 704, Generator loss: 1.2228997945785522; Student loss: 0.34591445326805115; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 705, Generator loss: 1.266129732131958; Student loss: 0.35368612408638; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 706, Generator loss: 1.2643506526947021; Student loss: 0.36335015296936035; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 707, Generator loss: 1.254368543624878; Student loss: 0.3514573872089386; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 708, Generator loss: 1.233642339706421; Student loss: 0.3376352787017822; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 709, Generator loss: 1.2265844345092773; Student loss: 0.35204362869262695; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 710, Generator loss: 1.2373430728912354; Student loss: 0.3515295386314392; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 711, Generator loss: 1.236966848373413; Student loss: 0.352240651845932; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 712, Generator loss: 1.2686351537704468; Student loss: 0.34873539209365845; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 713, Generator loss: 1.2564705610275269; Student loss: 0.34260642528533936; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 714, Generator loss: 1.2685246467590332; Student loss: 0.34617793560028076; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 715, Generator loss: 1.2642929553985596; Student loss: 0.3468545079231262; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 716, Generator loss: 1.2787880897521973; Student loss: 0.35517388582229614; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 717, Generator loss: 1.2478690147399902; Student loss: 0.34438106417655945; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 718, Generator loss: 1.2360893487930298; Student loss: 0.3411964476108551; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 719, Generator loss: 1.2546883821487427; Student loss: 0.34284594655036926; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 720, Generator loss: 1.2406611442565918; Student loss: 0.3528134822845459; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 721, Generator loss: 1.2342634201049805; Student loss: 0.3395272195339203; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 722, Generator loss: 1.257962703704834; Student loss: 0.34251850843429565; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 723, Generator loss: 1.2522377967834473; Student loss: 0.35870829224586487; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 724, Generator loss: 1.236621618270874; Student loss: 0.3442210555076599; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 725, Generator loss: 1.2220776081085205; Student loss: 0.3449937403202057; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 726, Generator loss: 1.234826922416687; Student loss: 0.34327036142349243; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 727, Generator loss: 1.2741912603378296; Student loss: 0.3524686396121979; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 728, Generator loss: 1.2705714702606201; Student loss: 0.3504217863082886; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 729, Generator loss: 1.245890736579895; Student loss: 0.35087135434150696; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 730, Generator loss: 1.221500039100647; Student loss: 0.34187713265419006; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 731, Generator loss: 1.235211730003357; Student loss: 0.33662882447242737; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 732, Generator loss: 1.2488741874694824; Student loss: 0.3491774797439575; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 733, Generator loss: 1.2364354133605957; Student loss: 0.34918633103370667; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 734, Generator loss: 1.261091947555542; Student loss: 0.34536612033843994; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 735, Generator loss: 1.2490699291229248; Student loss: 0.34178370237350464; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 736, Generator loss: 1.2318223714828491; Student loss: 0.34590592980384827; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 737, Generator loss: 1.2579529285430908; Student loss: 0.3424772620201111; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 738, Generator loss: 1.2518188953399658; Student loss: 0.34103500843048096; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 739, Generator loss: 1.2613885402679443; Student loss: 0.34311413764953613; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 740, Generator loss: 1.253088116645813; Student loss: 0.3367063105106354; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 741, Generator loss: 1.2390133142471313; Student loss: 0.3411661982536316; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 742, Generator loss: 1.2730474472045898; Student loss: 0.3374558985233307; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 743, Generator loss: 1.2635960578918457; Student loss: 0.34161341190338135; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 744, Generator loss: 1.2625505924224854; Student loss: 0.3446635603904724; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 745, Generator loss: 1.2743130922317505; Student loss: 0.33288800716400146; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 746, Generator loss: 1.2592774629592896; Student loss: 0.34570083022117615; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 747, Generator loss: 1.2365015745162964; Student loss: 0.34253358840942383; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 748, Generator loss: 1.2370898723602295; Student loss: 0.3424829840660095; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 749, Generator loss: 1.2727785110473633; Student loss: 0.3383759558200836; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 750, Generator loss: 1.2671682834625244; Student loss: 0.33896178007125854; Epsilon: 2784.04296875, r_sum: 0.0\n",
      "Step 751, Generator loss: 1.233839511871338; Student loss: 0.3446040153503418; Epsilon: 2784.04296875, r_sum: 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25796/3310791801.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpate_gan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcreditcard_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_teacher_iters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_student_iters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25796/1110977567.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X, batch_size, n_moments, lap_scale, n_teacher_iters, n_student_iters)\u001b[0m\n\u001b[0;32m     79\u001b[0m                 \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mz_dim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m                 \u001b[0mu_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                 \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclean_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mteachers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlap_scale\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m                 \u001b[1;31m# r = r + 0.05 * tf.random.uniform(tf.shape(r), dtype=tf.float32)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[1;31m# print(clean_results)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25796/1295507332.py\u001b[0m in \u001b[0;36mpate\u001b[1;34m(data, teachers, lap_scale)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mteachers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlap_scale\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     clean_results = tf.math.reduce_sum(tf.concat([\n\u001b[0m\u001b[0;32m      4\u001b[0m        \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mteacher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mteacher\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mteachers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     ], axis=1), axis=1)\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25796/1295507332.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     clean_results = tf.math.reduce_sum(tf.concat([\n\u001b[1;32m----> 4\u001b[1;33m        \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mteacher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mteacher\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mteachers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     ], axis=1), axis=1)\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\digic\\synthbench\\.synthbench\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\digic\\synthbench\\.synthbench\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1081\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1082\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1083\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1084\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1085\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\digic\\synthbench\\.synthbench\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[0mbound_signature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_keras_call_info_injected'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\digic\\synthbench\\.synthbench\\lib\\site-packages\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    371\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 373\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m  \u001b[1;31m# handle the corner case where self.layers is empty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\digic\\synthbench\\.synthbench\\lib\\site-packages\\keras\\engine\\functional.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    449\u001b[0m         \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtensors\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mthere\u001b[0m \u001b[0mare\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mthan\u001b[0m \u001b[0mone\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \"\"\"\n\u001b[1;32m--> 451\u001b[1;33m     return self._run_internal_graph(\n\u001b[0m\u001b[0;32m    452\u001b[0m         inputs, training=training, mask=mask)\n\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\digic\\synthbench\\.synthbench\\lib\\site-packages\\keras\\engine\\functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m         \u001b[1;31m# Update tensor_dict.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\digic\\synthbench\\.synthbench\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\digic\\synthbench\\.synthbench\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1081\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1082\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1083\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1084\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1085\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\digic\\synthbench\\.synthbench\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[0mbound_signature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_keras_call_info_injected'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\digic\\synthbench\\.synthbench\\lib\\site-packages\\keras\\layers\\core\\dense.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    197\u001b[0m             self.kernel, ids, weights, combiner='sum')\n\u001b[0;32m    198\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m     \u001b[1;31m# Broadcast kernel to inputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\digic\\synthbench\\.synthbench\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\digic\\synthbench\\.synthbench\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1094\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1095\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1096\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1097\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\digic\\synthbench\\.synthbench\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[1;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, output_type, name)\u001b[0m\n\u001b[0;32m   3698\u001b[0m             a, b, adj_x=adjoint_a, adj_y=adjoint_b, Tout=output_type, name=name)\n\u001b[0;32m   3699\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3700\u001b[1;33m         return gen_math_ops.mat_mul(\n\u001b[0m\u001b[0;32m   3701\u001b[0m             a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0;32m   3702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\digic\\synthbench\\.synthbench\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[1;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[0;32m   6010\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6011\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6012\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   6013\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"MatMul\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"transpose_a\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"transpose_b\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6014\u001b[0m         transpose_b)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pate_gan.train(creditcard_scaled, 64, 20, 0.5, n_teacher_iters=1, n_student_iters=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "p8mP_MRCCnkg"
   },
   "outputs": [],
   "source": [
    "synthetic = pate_gan.generate(1e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "i88naGBKnYA8"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the distribution of a random column\n",
    "\n",
    "col = np.random.randint(30)\n",
    "\n",
    "plt.hist(synthetic[:, col])\n",
    "plt.show()\n",
    "\n",
    "plt.hist(creditcard_scaled[:, col])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "VaYQzXfrGz7u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) Input with unsupported characters which will be renamed to input in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: PATE-GAN model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: PATE-GAN model\\assets\n"
     ]
    }
   ],
   "source": [
    "pate_gan.generator.save(\"PATE-GAN model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PATE-GAN Logistic Regression.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
